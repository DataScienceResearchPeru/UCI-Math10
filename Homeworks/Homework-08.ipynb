{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Set Week 8\n",
    "## insert your name here\n",
    "\n",
    "In this homework assignment please fill the indicated cells with your code and explainations, ***run*** everything (select `cell` in the menu, and click `Run all`), save the notebook with your name appended to the filename (for example, `Homework-08-caos.ipynb`), and upload it to canvas.\n",
    "\n",
    "This homework assignment studies and tests about the $k$NN classifier. This homework will prepare you for the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Weighted voting in kNN\n",
    "\n",
    "A simple and effective way to remedy skewed class distributions is by implementing weighed voting.\n",
    "\n",
    "Once we have obtained a new coming sample $\\mathbf{x}_0$'s $k$-nearest-neighbors using the chosen distance function $\\text{dist}(\\mathbf{x}_0, \\mathbf{x})$, these $k$ neighbors are going to \"vote\" in order to predict this sample's class. Two approaches are common:\n",
    "* Equal voting: for each class, we count how many of the $k$ neighbors are in that class. We classify this new sample into the class with the most \"votes\".\n",
    "* Inverse distance-weighted voting: closer neighbors get higher \"votes\". The class of each of the $k$ neighbors is multiplied by a weight proportional to the inverse of the distance from that point to the given test point. This ensures that nearer neighbors contribute more to the final vote than the more distant ones. To be specific: let the newcoming sample of interest be $\\mathbf{x}_0$, then the vote function $V(\\mathbf{x}_i)$ for $i=1,\\dots, k$ for these $k$ neighbors are defined as\n",
    "$$\n",
    "V(\\mathbf{x}_i) = \\begin{cases}\n",
    "\\infty & \\text{ if } \\text{dist}(\\mathbf{x}_0, \\mathbf{x}_i) = 0,\n",
    "\\\\[1em]\n",
    "\\displaystyle\\frac{1}{\\text{dist}(\\mathbf{x}_0, \\mathbf{x}_i)} & \\text{ otherwise }.\n",
    "\\end{cases}\n",
    "$$\n",
    "Then we sum the votes for each class among these $k$ neighbors and classify the newcoming sample $\\mathbf{x}_0$ into the class with the highest vote.\n",
    "\n",
    "\n",
    "### Question:\n",
    "Use the `load_iris` function in `scikit-learn.datasets` to load the Iris data. Split the data into 60% training and 40% testing samples. Implement the weighted voting in $k$NN following Lecture 21's notebook, either vectorized version or the `for` loop version is fine. \n",
    "\n",
    "Expected accuracy: 5 neighbors should yield an over-95% accuracy using this voting mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 (Optional): kNN with Hamming distance for MNIST\n",
    "\n",
    "The [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) between two vectors $\\mathbf{u} = (u_1,\\dots,u_n)$ and $\\mathbf{v} = (v_1,\\dots,v_n)$, is simply the proportion of disagreeing components in $\\mathbf{u}$ and $\\mathbf{v}$. To be specific, the Hamming distance is:\n",
    "$$\n",
    "\\text{HammingDist}(\\mathbf{u},\\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_H\n",
    "= \\frac{1}{n}\\sum_{i=1}^n 1\\{u_i \\neq v_i\\}\n",
    "$$\n",
    "\n",
    "Some examples are as follows, a special remark is that the distances in the last two examples are the same. This special property of Hamming distance is very nice in digit recognition because we are more likely to care about the pattern instead of the pixel intensities, in other words, the \"shape\"-complement of the digits (where the pixel intensities are zero) will contribute more than the actual pixel intensities.\n",
    "\n",
    "Reference: [Hamming Distance Metric Learning](https://papers.nips.cc/paper/4808-hamming-distance-metric-learning.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some example\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.hamming([1, 0, 0], [0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.hamming([1, 0, 0], [1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.hamming([1, 0, 0], [2, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.hamming([1, 0, 0], [3, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "* Import the binary data of MNIST (only zeros and ones) using the `mnist_binary_train.npz` and `mnist_binary_test.npz` on Canvas files.\n",
    "* Pre-process the data so that each training and testing sample have entries with only 0's and 1's. For example, we can re-assign every non-zero pixel intensities in the first training sample to be 1 by the following simple line:\n",
    "```python\n",
    "X_train[0,:] = 1.0*(X_train[0,:] > 0.0)\n",
    "```\n",
    "* For $k=5, 13, 21$, apply $k$NN with Hamming distance on the testing dataset to predict the testing sample represents a 1 or 0. Print the prediction accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
