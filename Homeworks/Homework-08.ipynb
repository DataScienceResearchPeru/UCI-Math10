{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Set Week 8\n",
    "## insert your name here\n",
    "\n",
    "In this homework assignment please fill the indicated cells with your code and explainations, ***run*** everything (select `cell` in the menu, and click `Run all`), save the notebook with your name appended to the filename (for example, `Homework-08-shuhaocao.ipynb`), and upload it to canvas.\n",
    "\n",
    "This homework assignment studies and tests about the stochastic gradient descent and the $k$NN classifier, we will use the MNIST dataset which can be download as zip/npz files either from Canvas or [from Kaggle](https://www.kaggle.com/oddrationale/mnist-in-csv). This homework will prepare you for the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: SGD for the softmax classifier on MNIST dataset\n",
    "\n",
    "In this problem we will test the performance of the SGD vs GD on the classification problem for the MNIST dataset. Please complete the following:\n",
    "* Import both the training and testing samples in the MNIST dataset using the following cell.\n",
    "* Use the softmax regression set-up in the cell that follows, model (input an image, output 10 probabilities), softmax loss function, and the gradient of the loss function.\n",
    "* (Graded)  Run 1 iteration of the gradient descent with the learning rate $\\eta = 10^{-5}$, and 1 epoch of the stochastic gradient descent for all the training samples with the learning rate $\\eta = 10^{-6}$. Compare the prediction accuracies of both approaches on the testing samples. When comparing, please either fix the random number generator seed, or use zero initial guess for both weights. A remark is that vanilla SGD may take some time since it is a `for` loop iterating 60,000 times, you may want to print the accuracy every 5000 iterations to make sure your SGD goes in the right direction.\n",
    "* (Graded) Try the mini-batch SGD to minimize the softmax loss function, and let the mini-batch size to be 32. Choose appropriate learning rate $\\eta$, number of inner iterations $M$, number of epochs (outer iterations) $n_E$, so that the algorithm converges and the prediction reaches a reasonable accuracy ($\\approx 90\\%$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load('mnist_train.npz')\n",
    "data_test = np.load('mnist_test.npz')\n",
    "X_train = data_train['X']\n",
    "y_train = data_train['y']\n",
    "X_test = data_test['X']\n",
    "y_test = data_test['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables:\n",
    "n = np.shape(X_train)[1] # 784, which is number of pixels (number of features)\n",
    "K = 10 # number of classes\n",
    "N = len(y_train) # number of training samples\n",
    "\n",
    "def sigma(w,X):\n",
    "    w = w.reshape(n,K)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1,-1)\n",
    "    s = np.exp(np.matmul(X,w))\n",
    "    total = np.apply_along_axis(np.sum, 1, s).reshape(-1,1)\n",
    "    prob = s / total\n",
    "    return prob\n",
    "\n",
    "def loss(w,X,y):\n",
    "    loss_samples = np.zeros(N)\n",
    "    for k in range(K):\n",
    "        loss_samples += np.log(sigma(w,X))[:,k] * (y == k)\n",
    "    return -np.mean(loss_samples) \n",
    "\n",
    "def gradient_loss(w,X,y):\n",
    "    gd_class_all = np.empty([n,K]) \n",
    "    for k in range(K):\n",
    "        gd_class_k = (sigma(w,X)[:,k] - (y==k)).reshape(-1,1)*X\n",
    "        gd_class_all[:,k] = np.mean(gd_class_k, axis=0)\n",
    "    return gd_class_all.reshape(n*K)\n",
    "\n",
    "def cross_validate_acc(w,X,y):\n",
    "    prob = sigma(w,X)\n",
    "    highest_prob_index = np.argmax(prob, axis=1)\n",
    "    return np.mean(highest_prob_index == y.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mini-batch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Weighted voting in kNN\n",
    "\n",
    "A simple and effective way to remedy skewed class distributions is by implementing weighed voting.\n",
    "\n",
    "Once we have obtained a new coming sample $\\mathbf{x}_0$'s $k$-nearest-neighbors using the chosen distance function $\\text{dist}(\\mathbf{x}_0, \\mathbf{x})$, these $k$ neighbors are going to \"vote\" in order to predict this sample's class. Two approaches are common:\n",
    "* Equal voting: for each class, we count how many of the $k$ neighbors are in that class. We classify this new sample into the class with the most \"votes\".\n",
    "* Inverse distance-weighted voting: closer neighbors get higher \"votes\". The class of each of the $k$ neighbors is multiplied by a weight proportional to the inverse of the distance from that point to the given test point. This ensures that nearer neighbors contribute more to the final vote than the more distant ones. To be specific: let the newcoming sample of interest be $\\mathbf{x}_0$, then the vote function $V(\\mathbf{x}_i)$ for $i=1,\\dots, k$ for these $k$ neighbors are defined as\n",
    "$$\n",
    "V(\\mathbf{x}_i) = \\begin{cases}\n",
    "\\infty & \\text{ if } \\text{dist}(\\mathbf{x}_0, \\mathbf{x}_i) = 0,\n",
    "\\\\[1em]\n",
    "\\displaystyle\\frac{1}{\\text{dist}(\\mathbf{x}_0, \\mathbf{x}_i)} & \\text{ otherwise }.\n",
    "\\end{cases}\n",
    "$$\n",
    "Then we sum the votes for each class among these $k$ neighbors and classify the newcoming sample $\\mathbf{x}_0$ into the class with the highest vote.\n",
    "\n",
    "\n",
    "### Question:\n",
    "Use the `load_iris` function in `scikit-learn.datasets` to load the Iris data. Split the data into 60% training and 40% testing samples. Implement the weighted voting in $k$NN following Lecture 21's notebook, either vectorized version or the `for` loop version is fine. \n",
    "\n",
    "Expected accuracy: 5 neighbors should yield a 100% accuracy using this voting mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 (Optional): kNN with Hamming distance for MNIST\n",
    "\n",
    "The [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) between two vectors $\\mathbf{u} = (u_1,\\dots,u_n)$ and $\\mathbf{v} = (v_1,\\dots,v_n)$, is simply the proportion of disagreeing components in $\\mathbf{u}$ and $\\mathbf{v}$. To be specific, the Hamming distance is:\n",
    "$$\n",
    "\\text{HammingDist}(\\mathbf{u},\\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_H\n",
    "= \\frac{1}{n}\\sum_{i=1}^n 1\\{u_i \\neq v_i\\}\n",
    "$$\n",
    "\n",
    "Some examples are as follows, a special remark is that the distances in the last two examples are the same. This special property of Hamming distance is very nice in digit recognition because we are more likely to care about the pattern instead of the pixel intensities, in other words, the \"shape\"-complement of the digits (where the pixel intensities are zero) will contribute more than the actual pixel intensities.\n",
    "\n",
    "Reference: [Hamming Distance Metric Learning](https://papers.nips.cc/paper/4808-hamming-distance-metric-learning.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some example\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.hamming([1, 0, 0], [0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.hamming([1, 0, 0], [1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.hamming([1, 0, 0], [2, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.hamming([1, 0, 0], [3, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "* Import the binary data of MNIST (only zeros and ones) using the `mnist_binary_train.npz` and `mnist_binary_test.npz` on Canvas files.\n",
    "* Pre-process the data so that each training and testing sample have entries with only 0's and 1's. For example, we can re-assign every non-zero pixel intensities in the first training sample to be 1 by the following simple line:\n",
    "```python\n",
    "X_train[0,:] = 1.0*(X_train[0,:] > 0.0)\n",
    "```\n",
    "* For $k=5, 13, 21$, apply $k$NN with Hamming distance on the testing dataset to predict the testing sample represents a 1 or 0. Print the prediction accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
