{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 16: Linear Regression by Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today let us learn to minimize the loss function in the linear regression using gradient descent.\n",
    "\n",
    "Today we will import the wine quality data from [UCI machine learning dataset repo on Kaggle](https://www.kaggle.com/uciml/datasets) like we did in last lecture.\n",
    "\n",
    "* Download `winequality-red.csv` from [https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009/), unzip it and put it in the same directory with this notebook.\n",
    "* Check the csv file using Excel on the lab computer. Be careful with the delimiters, Kaggle's csv is using `,` comma as delimiter. Import the data using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will help us plot things nicely\n",
    "def plot_data_and_line(weight, bias, X, Y):\n",
    "    # this function plots data against our model\n",
    "    # weight and bias are our regression model's parameters\n",
    "    # Conventionally X are our training data, Y are the labels \n",
    "    plot_max, plot_min = max(X), min(X)\n",
    "    XX = np.linspace(plot_min,plot_max,200)\n",
    "    YY = weight*XX + bias\n",
    "\n",
    "    plt.scatter(X, Y, alpha=0.1)\n",
    "    plt.plot(XX, YY, color='red',linewidth = 4, alpha=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = np.loadtxt('winequality-red.csv', delimiter=',', skiprows=1, usecols = [0,1,2])\n",
    "fix_acid = wine_data[:,0]\n",
    "vol_acid = wine_data[:,1]\n",
    "ctr_acid = wine_data[:,2]\n",
    "\n",
    "# some pre-processing by normalizing the data to have zero mean\n",
    "fix_acid = fix_acid - np.mean(fix_acid)\n",
    "vol_acid = vol_acid - np.mean(vol_acid)\n",
    "ctr_acid = ctr_acid - np.mean(ctr_acid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the Loss function we want to minimize: \n",
    "$$L(w,b) = \\frac{1}{N}\\sum_{i=1}^{N} \\Big((w x_i + b) - y_i\\Big)^2,$$\n",
    "the $1/N$ factor is added so that statistically we are trying to minimize an expected value (sample mean) from an empirical distribution.\n",
    "Here we want to investigate the relation of citric acid as $x_i$ with the fixed acidity as $y_i$. We want to form a linear function $f(x_i; w, b) = w x_i + b$ using citric acid as input, $w,b$ as parameter, to as close to $y_i$ (actual data) as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ctr_acid\n",
    "y = fix_acid\n",
    "N = len(x)\n",
    "\n",
    "# our model\n",
    "def f(w, b):\n",
    "    # x is the data, which is globally assigned in the memory\n",
    "    return w * x + b\n",
    "\n",
    "# loss function = total square error on the whole data set\n",
    "def loss(w, b):\n",
    "    # x,y globally assigned outside this function for simplicity\n",
    "    error = f(w,b) - y\n",
    "    return (1/N)*np.sum(error**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical partial derivatives from Lecture 14\n",
    "def numpartialx(f):\n",
    "    h = 0.0000001 \n",
    "    return (lambda x, y: (f(x+h, y)-f(x, y))/h)\n",
    "\n",
    "def numpartialy(f):\n",
    "    h = 0.0000001 \n",
    "    return (lambda x, y: (f(x, y+h)-f(x, y))/h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent routine copied from Lecture 14\n",
    "\n",
    "> Choose initial guess $(w_0,b_0)$ and step size (learning rate) $\\eta$<br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br><br>\n",
    ">     &nbsp;&nbsp;&nbsp;&nbsp;   $(w_{k+1},b_{k+1}) =  (w_k,b_k) - \\eta\\nabla L(w_k,b_k) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eta = 0.05  # step size (learning rate)\n",
    "num_steps = 600\n",
    "\n",
    "# initial guess\n",
    "w = 0.0\n",
    "b = 2.0\n",
    "loss_at_eachstep = np.zeros(num_steps) # record the change of the loss function\n",
    "for i in range(num_steps):\n",
    "    loss_at_eachstep[i] = loss(w,b)\n",
    "    dw, db = numpartialx(loss)(w,b), numpartialy(loss)(w,b)\n",
    "    w = w - eta * dw\n",
    "    b = b - eta * db\n",
    "    if i in (1, 2, 5, 10, 20, 50, 100, 200, 400):\n",
    "        plot_data_and_line(w, b, x, y)\n",
    "\n",
    "print(\"final loss after\", num_steps, \"iterations is: \", loss(w,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_steps), loss_at_eachstep)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise\n",
    "* Try the same procedure by letting `x = vol_acid` and `y = fix_acid`.\n",
    "* Try varying the step size (learning rate) $\\eta$ from 1, 1e-1, 1e-2, to 1e-3. What have you observed?\n",
    "* A regularization can be added to the loss function to make the converging process smoother: we can choose a smaller $\\epsilon \\ll 1$ such that our loss is \"regularized\" by the squared (sum) of the weight(s) (bias not included)\n",
    "$$L(w,b) = \\frac{1}{N}\\sum_{i=1}^{N} \\Big((w x_i + b) - y_i\\Big)^2 + \\epsilon w^2$$\n",
    "Try this with $\\epsilon = 10^{-3}$,\n",
    "(Reading: this is called [Ridge regression](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
