{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 18: Logistic regression, binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last few lectures we have learned the linear regression, where we explore the possibility of using a linear function to represent the relation of the data ($x$ values, or training data `X_train`) and a label ($y$ values `y_train`), so that we can predict the label $y$ (`y_pred`) using testing data `X_test`.\n",
    "\n",
    "Today, we will learn how to predict a discrete label such as \n",
    "* predicting whether a grid of pixel intensities represents a \"0\" digit or a \"1\" digit;\n",
    "* predicting whether tomorrow will have rain based on previous days' data.\n",
    "\n",
    "This is a classification problem. Logistic regression is a simple classification algorithm for learning to make such decisions for a binary label.\n",
    "\n",
    "Reference: adapted from the MATLAB tutorial in [Stanford Deep Learning tutorial](http://deeplearning.stanford.edu/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "Now let us look at the famous [MNIST dataset of handwritten digits](http://yann.lecun.com/exdb/mnist/), we should download the zip file on Canvas or the [csv files from Kaggle](https://www.kaggle.com/oddrationale/mnist-in-csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.loadtxt('mnist_train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the data look like?\n",
    "The first column `data_train[:,0]` is the label, and the rest 784 columns `data_train[:,1:]` represent the image. Let us try to visualize the first 20 rows of the training data, with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train[:,1:]\n",
    "y = data_train[:,0]\n",
    "\n",
    "fig, axes = plt.subplots(4,5, figsize=(12, 10))\n",
    "axes = axes.reshape(-1)\n",
    "\n",
    "for i in range(20):\n",
    "    axes[i].axis('off') # hide the axes ticks\n",
    "    axes[i].imshow(X[i,:].reshape(28,28), cmap = 'gray')\n",
    "    axes[i].set_title(str(int(y[i])), color= 'black', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of the training data\n",
    "Now let us extract 0's and 1's from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_and_one = (y == 1) + (y == 0)\n",
    "zero_and_one_indx = np.nonzero(zero_and_one)\n",
    "zero_and_one_indx = zero_and_one_indx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[zero_and_one_indx,:]\n",
    "y_train = y[zero_and_one_indx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double check if it is really 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,5, figsize=(12, 5))\n",
    "axes = axes.reshape(-1)\n",
    "for i in range(10):\n",
    "    axes[i].axis('off') # hide the axes ticks\n",
    "    axes[i].imshow(X_train[i,:].reshape(28,28), cmap = 'gray')\n",
    "    axes[i].set_title(str(int(y_train[i])), color= 'black', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (reading)\n",
    "In linear regression we tried to predict the value of $y^{(i)}$ for the $i$-th sample $\\mathbf{x}^{(i)}$ using a linear function $y=h(\\mathbf{x}) = \\mathbf{w}^{\\top} [1, \\; \\mathbf{x}] $. This is clearly not a great solution for predicting wine quality score (which we can classify as \"favorable\" and \"unfavorable\"). \n",
    "\n",
    "For binary-valued labels, e.g., $y^{(i)} \\in \\{0,1\\}$, we can use **logistic regression** to try to predict the probability that a given example belongs to the \"1\" class versus the probability that it belongs to the \"0\" class. Specifically, we will try to learn a function of the form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(\\mathbf{x}) = P(y=1|\\mathbf{x}) & = \\frac{1}{1 + \\exp(-\\mathbf{w}^\\top \\mathbf{x})}\n",
    "=: \\sigma(\\mathbf{w}^\\top \\mathbf{x})  ,\\\\[5pt]\n",
    "1 - h(\\mathbf{x}) = P(y=0|\\mathbf{x}) &= 1 - P(y=1|\\mathbf{x}) ,\n",
    "\\end{aligned}\n",
    "$$\n",
    "or more compactly, because $y = 0$ or $1$:\n",
    "$$\n",
    "P(y|\\mathbf{x}) = h(\\mathbf{x})^y \\big(1 - h(\\mathbf{x}) \\big)^{1-y}\n",
    "$$\n",
    "\n",
    "----\n",
    "\n",
    "## Sigmoid function\n",
    "The function $\\sigma(z) = 1/\\big(1+\\exp(−z)\\big)$ is often called the \"sigmoid\" or \"logistic\" function, or \"logistic/sigmoid\" activation function in machine learning. It is an S-shaped function that \"squashes\" the value of $\\mathbf{w}^\\top \\mathbf{x}$ into the range $[0,1]$ so that we may interpret $h(\\mathbf{x})$ as a probability. Our goal is to search for a value of $\\mathbf{w}$ so that:\n",
    "> The probability $P(y=1|\\mathbf{x})=h(\\mathbf{x})$ is large when $x$ belongs to the \"1\" class, small when $x$ belongs to the \"0\" class (so that $P(y=0|\\mathbf{x})=1- h(\\mathbf{x})$ is large). \n",
    "\n",
    "----\n",
    "\n",
    "## Maximum likelihood\n",
    "For a set of training examples with binary labels $\\{(\\mathbf{x}^{(i)},y^{(i)}):i=1,\\dots,N\\}$ the following likelihood estimator measures how well a given $h(\\mathbf{x})$ does this: assuming our training samples are independently Bernoulli distributed, we want to maximize the following quantity\n",
    "$$\n",
    "{\\begin{aligned}L(\\mathbf{w} | \\mathbf{x})&=P(\\mathbf{y}\\; | \\; \\mathbf{X};\\mathbf{w} )\\\\\n",
    "&=\\prod _{i=1}^N P\\left(y^{(i)}\\mid \\mathbf{x}^{(i)};\\mathbf{w}\\right)\\\\\n",
    "&=\\prod_{i=1}^N h\\big(\\mathbf{x}^{(i)} \\big)^{y_{i}} \\Big(1-h\\big(\\mathbf{x}^{(i)}\\big) \\Big)^{(1-y_{i})}\n",
    "\\end{aligned}}.\n",
    "$$\n",
    "This function is highly nonlinear on the weights $\\mathbf{w}$ so we take the log and define our loss function to be minimized as follows:\n",
    "$$\n",
    "\\ell (\\mathbf{w}) = - \\sum_{i=1}^N \n",
    "\\Bigl\\{y^{(i)} \\ln\\big( h(\\mathbf{x}^{(i)}) \\big) \n",
    "+ (1 - y^{(i)}) \\ln\\big( 1 - h(\\mathbf{x}^{(i)}) \\big) \\Bigr\\}.\n",
    "\\tag{$\\star$}\n",
    "$$\n",
    "Note that only one of the two terms in the summation is non-zero for each training sample (depending on whether the label $y^{(i)}$ is 0 or 1). When $y^{(i)}=1$ minimizing the loss function means we need to make $h(x^{(i)})$ large, and when $y^{(i)}= 0$ we want to make $1- h(x^{(i)})$ large as explained above. \n",
    "\n",
    "----\n",
    "\n",
    "## Training and cross-validation\n",
    "After the loss function $\\ell (\\mathbf{w})$ is set up, the training data is used by the gradient descent to minimize $\\ell (\\mathbf{w})$ to find the best choice of weights $\\mathbf{w}$. Even though the cost function $(\\star)$ looks quite complicated, due to the following special property of the Sigmoid function \n",
    "$$\n",
    "\\frac{d}{dz} \\big(\\sigma(z)\\big)\n",
    " = \\frac{d}{dz} \\left(\\frac{1}{1+\\exp(−z)}\\right) = \\sigma(z)\\cdot \\big(1-\\sigma(z)\\big).\n",
    "$$\n",
    "Therefore recalling $h(\\mathbf{x}) =  \\sigma(\\mathbf{w}^\\top \\mathbf{x})$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\ell (\\mathbf{w})}{\\partial w_k} & = \n",
    "- \\sum_{i=1}^N \n",
    "\\Bigg\\{y^{(i)}  \\frac{1}{h(\\mathbf{x}^{(i)})} \\frac{\\partial}{\\partial w_k} \\sigma\\big(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}  \\big)\n",
    "+ (1 - y^{(i)}) \\frac{1}{1-h(\\mathbf{x}^{(i)})} \\frac{\\partial}{\\partial w_k}\\Big(1-  \\sigma\\big(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}\\big)  \\Big) \\Bigg\\}\n",
    "\\\\\n",
    "& = - \\sum_{i=1}^N \n",
    "\\Bigg\\{y^{(i)}  \\frac{1}{h(\\mathbf{x}^{(i)})} \n",
    "\\sigma\\big(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}\\big) \n",
    "\\cdot \\big(1-\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)})\\big) \n",
    "\\frac{\\partial}{\\partial w_k} \\sigma\\big(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}  \\big)\n",
    "\\\\\n",
    "& \\qquad \\qquad - (1 - y^{(i)}) \\frac{1}{1-h(\\mathbf{x}^{(i)})} \n",
    "\\sigma\\big(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}\\big) \n",
    "\\cdot \\big(1-\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)})\\big) \n",
    "\\frac{\\partial}{\\partial w_k}\\big(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}\\big)  \\Bigg\\}\n",
    "\\\\\n",
    "& = - \\sum_{i=1}^N \n",
    "\\Bigg\\{y^{(i)} \\cdot \\big(1-\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)})\\big) \n",
    "\\frac{\\partial}{\\partial w_k} \\sigma\\big(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}  \\big)\n",
    "- (1 - y^{(i)}) \\cdot\n",
    "\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}) \n",
    "\\frac{\\partial}{\\partial w_k}\\big(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}\\big)  \\Bigg\\}\n",
    "\\\\\n",
    "& =\\sum_{i=1}^N  \\big(\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)})  - y^{(i)} \\big) x^{(i)}_k.\n",
    "\\end{aligned}\n",
    "$$\n",
    "The final expression is pretty simple, basically the derivative of the Logistic loss function w.r.t. the $k$-th weight $w_k$ is the sum of the residuals $\\big(\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)})  - y^{(i)} \\big) $ multiply with the $k$-th component in the $i$-th training data $\\mathbf{x}^{(i)}$.\n",
    "\n",
    "Therefore the gradient for all the weights $\\mathbf{w}$ is then:\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\big( \\ell (\\mathbf{w}) \\big) = \\sum_{i=1}^N  \\big(\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)})  - y^{(i)} \\big) \\mathbf{x}^{(i)} \n",
    "=\\sum_{i=1}^N \\big( h(\\mathbf{x}^{(i)})  - y^{(i)} \\big) \\mathbf{x}^{(i)}  . \\tag{$\\dagger$}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "Prove for the Sigmoid function $\\sigma(z)$: \n",
    "$\\displaystyle\\frac{d}{dz} \\big(\\sigma(z)\\big) = \\sigma(z)\\cdot \\big(1-\\sigma(z)\\big).\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Now let us run the gradient descent based on $(\\dagger)$, with code adapted from Lecture 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global training data\n",
    "x = X_train\n",
    "y = y_train\n",
    "N = len(y)\n",
    "w  = np.zeros(np.shape(x)[1]) \n",
    "# zero initial guess, np.shape(x)[1] = 784, which is no. of pixels\n",
    "# and we want it to be small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model h(x; w)\n",
    "# w: weights\n",
    "# x: training data\n",
    "# x has shape 12665 (no. of samples) row by 784 (no. of features)\n",
    "# w has shape 784\n",
    "def h(w,x):\n",
    "    z = np.matmul(x,w)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "# loss function, modulo by N (size of training data), a vectorized implementation without for loop\n",
    "def loss(w,x,y):\n",
    "    loss_components = np.log(h(w,x)) * y + (1.0 - y)* np.log(1 - h(w,x))\n",
    "    # above is a dimension (12665,) array\n",
    "    return -np.mean(loss_components) # same with loss_components.sum()/N\n",
    "\n",
    "def gradient_loss(w,x,y):\n",
    "    gradient_for_all_training_data = (h(w,x) - y).reshape(-1,1)*x\n",
    "    # we should return a (784,) array, which is averaging all 12655 training data\n",
    "    return np.mean(gradient_for_all_training_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.reshape(-1,1) trick is often used in vectorized programming in python\n",
    "np.arange(10).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check the loss function and its gradient, if they are too big...we are in trouble\n",
    "loss(w,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(gradient_loss(w,x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 5e-5  # step size (learning rate)\n",
    "num_steps = 500\n",
    "\n",
    "# this block is for plot\n",
    "fig, axes = plt.subplots(2,5, figsize=(12, 5))\n",
    "axes = axes.reshape(-1)\n",
    "\n",
    "\n",
    "loss_at_eachstep = np.zeros(num_steps) # record the change of the loss function\n",
    "for i in range(num_steps):\n",
    "    loss_at_eachstep[i] = loss(w,x,y)\n",
    "    dw = gradient_loss(w,x,y)\n",
    "    w = w - eta * dw\n",
    "    if i % 50 == 0: # plot weights and print loss every 50 steps\n",
    "        print(\"loss after\", i+1, \"iterations is: \", loss(w,x,y))\n",
    "        axes[i//50].axis('off')\n",
    "        axes[i//50].imshow(w.reshape(28,28), cmap = 'gray')\n",
    "        axes[i//50].set_title(\"%4i iterations\" % i)\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_steps), loss_at_eachstep)\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation (Judgement day)\n",
    "Now let us use the testing data set to see if the the accuracy is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the testing data and extract zeros and ones like we did before\n",
    "data_test = np.loadtxt('mnist_test.csv', delimiter=',')\n",
    "X_test_total = data_test[:,1:]\n",
    "y_test_total = data_test[:,0]\n",
    "zero_and_one = (y_test_total == 1) + (y_test_total == 0) \n",
    "zero_and_one_indx = np.nonzero(zero_and_one)\n",
    "zero_and_one_indx = zero_and_one_indx[0]\n",
    "X_test = X_test_total[zero_and_one_indx,:]\n",
    "y_test = y_test_total[zero_and_one_indx].astype(int) # we want our label to be an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the y_pred using the weights w and X_test\n",
    "sigmoid_based_on_w = h(w,X_test)\n",
    "y_pred = 1*(sigmoid_based_on_w > 0.5) # integer\n",
    "# if sigmnoid is > 0.5, it is the 2nd class (one), otherwise it is the first class (zero)\n",
    "percentage_getting_label_correct = np.mean(y_pred == y_test)\n",
    "print(\"{:.5%}\".format(percentage_getting_label_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise:\n",
    "Read the manual of the [logistic regression class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) in `scikit-learn`, follow the example there to redo the classification above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
