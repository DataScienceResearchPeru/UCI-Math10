{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19: Softmax regression, multiclass classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lecture we have learned the logistic regression to classify \"0\" digit or a \"1\" digit based on pixel intensities on a 28x28 grid.\n",
    "\n",
    "Today we will learn how to classify all 10 digits.\n",
    "\n",
    "Reference: adapted from the MATLAB tutorial in [Stanford Deep Learning tutorial](http://deeplearning.stanford.edu/tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "Let us load the [MNIST dataset of handwritten digits](http://yann.lecun.com/exdb/mnist/), both testing and training data. You can download the `npz` format file on Canvas file tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the data look like?\n",
    "* (If you use the same routine with Lecture 18 to load the `csv` data) The first column of `data_train[:,0]` and `data_test[:,0]` are the labels, and the rest 784 columns `data_train[:,1:]` and `data_test[:,1:]`represent a 28x28 grayscale image. \n",
    "* If you have loaded the `npz` data (numpy native format), `X_train` and `X_test` both have 784 columns which represent a 28x28 grayscale image. `y_train` and `y_test`, which range from 0 to 9 total 10 classes, are the labels of the training samples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('mnist_train', X=X_train, y=y_train)\n",
    "np.savez_compressed('mnist_test', X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load('mnist_train.npz')\n",
    "X_train = data_train['X']\n",
    "y_train = data_train['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load('mnist_test.npz')\n",
    "X_test = data_test['X']\n",
    "y_test = data_test['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first 20 rows of the training data, with their labels.\n",
    "fig, axes = plt.subplots(4,5, figsize=(12, 10))\n",
    "axes = axes.reshape(-1)\n",
    "\n",
    "for i in range(20):\n",
    "    axes[i].axis('off') # hide the axes ticks\n",
    "    axes[i].imshow(X_train[i,:].reshape(28,28), cmap = 'gray')\n",
    "    axes[i].set_title(str(int(y_train[i])), color= 'black', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax regression (reading)\n",
    "<br>\n",
    "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: $y^{(i)}âˆˆ{0,1}$. In the last lecture, we have used such a classifier to distinguish between two kinds of handwritten digits. Softmax regression allows us to handle $y^{(i)}\\in \\{1,\\dots, K\\}$ where $K$ is the number of classes.\n",
    "\n",
    "Given a test input $\\mathbf{x}\\in \\mathbb{R}^n$ (a 28x28 image flattened to a `(784,)` array), we want to estimate the probability that $P(y=k|\\mathbf{x})$ for each value of $k=1,\\dots,K$. In other words, from the input image, we want to estimate the probability of this image being classified as each label among $K$ labels, and we choose the highest probable one to label this image. Thus, our model (hypothesis) will output a $K$-dimensional vector (whose elements sum to $1$ to make it a probability) giving us our $K$ estimated probabilities. Concretely, our model $h(\\mathbf{x}; \\mathbf{w})$, which stands for given the current weights $\\mathbf{w}$ the probability vector for $\\mathbf{x}$, takes the form:\n",
    "\n",
    "$$\n",
    "h_{\\mathbf{w}}(\\mathbf{x}) =\n",
    "\\begin{pmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w}) \\\\\n",
    "P(y = 2 | \\mathbf{x}; \\mathbf{w}) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = K | \\mathbf{x}; \\mathbf{w})\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{j=1}^{K}{\\exp\\big(\\mathbf{w}_j^{\\top} \\mathbf{x}\\big) }}\n",
    "\\begin{pmatrix}\n",
    "\\exp(\\mathbf{w}_1^{\\top} \\mathbf{x} ) \\\\\n",
    "\\exp(\\mathbf{w}_2^{\\top} \\mathbf{x} ) \\\\\n",
    "\\vdots \\\\\n",
    "\\exp(\\mathbf{w}_K^{\\top} \\mathbf{x} ) \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "Totally we have $K$ sets of parameters, $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K$, and the factor $\\sum_{j=1}^{K}{\\exp\\big(\\mathbf{w}_j^{\\top} \\mathbf{x}\\big)}$ normalizes the results to be a probability.\n",
    "\n",
    "When we implement the softmax regression, it is usually convenient to represent $\\mathbf{w}$ containing all $K$ sets of parameters as a $n\\times K$ matrix obtained by concatenating $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K$ into columns, so that $\\mathbf{w}_k = (w_{k1}, \\dots, w_{kn})^{\\top} = (w_{kl})$ for $l = 1,\\dots, n$\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\left(\n",
    "\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_K \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "and $\\mathbf{w}^{\\top}\\mathbf{x}$ would be sensible and vectorized to be computed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "Before introducting the loss the function for the softmax regression, we define the following indicator function:\n",
    "$$\n",
    "1_{\\{y = k\\}} = 1_{\\{k\\}}(y) = \\delta_{yk} = \\begin{cases}\n",
    "1 & \\text{when } y = k,\n",
    "\\\\[5pt]\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "First let us recall the loss function for the logistic regression, and we rewrite it as: we have $N$ training samples $(\\mathbf{x}^{(i)}, y^{(i)})$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell^{\\text{Logistic}} (\\mathbf{w}) &= - \\sum_{i=1}^N \n",
    "\\Bigl\\{y^{(i)} \\ln\\big( h(\\mathbf{x}^{(i)}) \\big) \n",
    "+ (1 - y^{(i)}) \\ln\\big( 1 - h(\\mathbf{x}^{(i)}) \\big) \\Bigr\\}\n",
    "\\\\\n",
    "& = - \\sum_{i=1}^N \\sum_{k=0}^1\n",
    "\\Bigl\\{ 1_{\\{y^{(i)} = k\\}} \\ln P\\big(y^{(i)}=k | \\mathbf{x}^{(i)} ; \\mathbf{w} \\big) \\Bigr\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now our loss function for the softmax regression is then the generalization of above:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L (\\mathbf{w}) = L^{\\text{Softmax}} (\\mathbf{w}) & = - \\sum_{i=1}^N \\sum_{k=1}^K\n",
    "\\Bigl\\{ 1_{\\{y^{(i)} = k\\}} \\ln P\\big(y^{(i)}=k | \\mathbf{x}^{(i)} ; \\mathbf{w} \\big) \\Bigr\\}\n",
    "\\\\\n",
    " & = - \\sum_{i=1}^N \\sum_{k=1}^K\n",
    "\\left\\{1_{\\{y^{(i)} = k\\}} \\ln \\Bigg( \\frac{\\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})}{\\sum_{j=1}^{K} \n",
    "\\exp\\big(\\mathbf{w}_j^{\\top} \\mathbf{x}^{(i)} \\big) }  \\Bigg)\\right\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Notice for every term in the sum w.r.t. to the labels, $\\sum_{k=1}^K$, $1_{\\{y^{(i)} = k\\}} = 1$ for only one term among $K$ terms, and the rest is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient of softmax loss function \n",
    "# (you might want to  re-derive this on paper)\n",
    "\n",
    "Notice our weights to be trained have $K$ sets $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K$, and each of the $k$-th weights vector has $n$ components: $\\mathbf{w}_k = (w_{k1}, \\dots, w_{kl},\\dots, w_{kn})^{\\top}$. The first subscript is $1\\leq k \\leq K$ (label's index, we have this many set of weights), the second subscript is $1\\leq l\\leq n$ ($\\mathbf{x}$'s feature index). \n",
    "\n",
    "The indices involved are pretty complicated, to simplify the notation a bit, denote the probability predicted by our model of the $i$-th training sample being in the $k$-th class as:\n",
    "\n",
    "$$\n",
    "\\sigma_{k}^{(i)}:= P\\big(y^{(i)}=k | \\mathbf{x}^{(i)} ; \\mathbf{w} \\big) = \n",
    "\\frac{\\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})} {\\sum_{m=1}^{K} \n",
    "\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)} )}\n",
    "$$\n",
    "then \n",
    "$$\n",
    "\\frac{\\partial L }{\\partial w_{jl}}\n",
    "= - \\sum_{i=1}^N \\sum_{k=1}^K \n",
    "\\left\\{ 1_{\\{y^{(i)} = k\\} } \\frac{\\partial}{\\partial w_{jl}}\\Big( \\ln \\sigma_{k}^{(i)}\\Big)\n",
    "\\right\\}\n",
    "= - \\sum_{i=1}^N \\sum_{k=1}^K \n",
    "\\left\\{ 1_{\\{y^{(i)} = k\\} } \\frac{1}{\\sigma_{k}^{(i)}}\\frac{\\partial}{\\partial w_{jl}} \\sigma_{k}^{(i)}\n",
    "\\right\\}.\n",
    "\\tag{$\\star$}\n",
    "$$\n",
    "\n",
    "Now computing the partial derivative above:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\sigma_{k}^{(i)}}{\\partial w_{jl}} \n",
    "&= \n",
    "\\frac{\\partial }{\\partial w_{jl}} \\left( \\frac{\\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})} {\\sum_{m=1}^{K} \n",
    "\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)} )}\\right)\n",
    "\\\\\n",
    "&= \\frac{1}{\\sum_{m=1}^{K}\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)} )}\n",
    "\\frac{\\partial }{\\partial w_{jl}} \\left(  \\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})\\right)\n",
    "- \\frac{\\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})}\n",
    "{ \\left(\\sum_{m=1}^{K}\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)}) \\right)^2}\n",
    "\\frac{\\partial }{\\partial w_{jl}} \\left(  \\sum_{m=1}^{K}\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)})  \\right)\n",
    "\\\\\n",
    "&= \\frac{1}{\\sum_{m=1}^{K}\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)} )}\n",
    "1_{\\{j = k\\}} \\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})\n",
    "\\frac{\\partial }{\\partial w_{jl}} \\left( \\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)} \\right)\n",
    "- \\frac{\\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})}\n",
    "{ \\left(\\sum_{m=1}^{K}\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)}) \\right)^2}\n",
    "\\exp(\\mathbf{w}_j^{\\top} \\mathbf{x}^{(i)})\n",
    "\\frac{\\partial }{\\partial w_{jl}} \\left( \\mathbf{w}_j^{\\top} \\mathbf{x}^{(i)} \\right).\n",
    "\\end{aligned}\n",
    "\\tag{$\\dagger$}\n",
    "$$\n",
    "\n",
    "By the property of the indicator function, we have:\n",
    "\n",
    "$$\n",
    "1_{\\{j = k\\}} \\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})\n",
    "\\frac{\\partial }{\\partial w_{jl}} \\left( \\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)} \\right)\n",
    "= \\begin{cases}\n",
    "\\exp(\\mathbf{w}_j^{\\top} \\mathbf{x}^{(i)}) x_l^{(i)} & \\text{if } j=k,\n",
    "\\\\[3pt]\n",
    "0 & \\text{if }j\\neq k.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Hence, $(\\dagger)$ can be further written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\sigma_{k}^{(i)}}{\\partial w_{jl}}  \n",
    "= \\frac{\\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})}\n",
    "{ \\sum_{m=1}^{K}\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)}) }\n",
    "\\left(\n",
    "1_{\\{j = k\\}} - \n",
    " \\frac{\\exp(\\mathbf{w}_j^{\\top} \\mathbf{x}^{(i)})}\n",
    "{ \\sum_{m=1}^{K}\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)}) }\n",
    "\\right) x^{(i)}_l\n",
    "= \\sigma_{k}^{(i)} \\left(\n",
    "1_{\\{j = k\\}} - \\sigma_{j}^{(i)} \\right) x^{(i)}_l.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now plugging this back to $(\\star)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L }{\\partial w_{jl}}\n",
    "&= - \\sum_{i=1}^N \\sum_{k=1}^K \n",
    "\\left\\{ 1_{\\{y^{(i)} = k\\} } \n",
    "\\left(\n",
    "1_{\\{j = k\\}} - \\sigma_{j}^{(i)} \\right) x^{(i)}_l\n",
    "\\right\\}\n",
    "\\\\\n",
    "&=- \\sum_{i=1}^N \n",
    "x^{(i)}_l\\left\\{ \\sum_{k=1}^K  \n",
    "1_{\\{y^{(i)} = k\\} } 1_{\\{j = k\\}} - \n",
    "\\sum_{k=1}^K 1_{\\{y^{(i)} = k\\} } \\sigma_{j}^{(i)}  \n",
    "\\right\\}\n",
    "\\\\\n",
    "&=\n",
    "- \\sum_{i=1}^N \n",
    "x^{(i)}_l\\left(   \n",
    "1_{\\{y^{(i)} = j\\} } -  \\sigma_{j}^{(i)}  \n",
    "\\right)\n",
    "\\\\\n",
    "& = \n",
    "- \\sum_{i=1}^N \n",
    "x^{(i)}_l\\Big(   \n",
    "1_{\\{y^{(i)} = j\\} } -  P\\big(y^{(i)}=j | \\mathbf{x}^{(i)} ; \\mathbf{w} \\big)  \n",
    "\\Big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is pretty simple, and it has a nice interpretation similar to the maximum likelihood function: the term in the parenthesis is the difference between the actual probability and the probability estimation in our model.\n",
    "\n",
    "Now the derivative of $L$ with respect the whole $j$-th set of weights is then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L }{\\partial \\mathbf{w}_{j}}\n",
    "= - \\sum_{i=1}^N \n",
    "\\mathbf{x}^{(i)}\\left(   1_{\\{y^{(i)} = j\\} } -  \\sigma_{j}^{(i)}  \n",
    "\\right)\n",
    "=\n",
    "\\sum_{i=1}^N \n",
    "\\mathbf{x}^{(i)}\\left(    \\frac{\\exp(\\mathbf{w}_j^{\\top} \\mathbf{x}^{(i)})} {\\sum_{m=1}^{K} \n",
    "\\exp(\\mathbf{w}_m^{\\top} \\mathbf{x}^{(i)} )} -1_{\\{y^{(i)} = j\\} } \n",
    "\\right).\n",
    "\\tag{$\\diamond$}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "Now let us implement the gradient descent based on $(\\diamond)$ above, with code adapted from Lecture 18. One big challenge here is that the weights are now represented by a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global training data\n",
    "X = X_train\n",
    "y = y_train\n",
    "N = len(y) # number of training samples\n",
    "n = np.shape(X)[1] # 784, which is number of pixels (number of features)\n",
    "K = 10 # number of classes\n",
    "np.random.seed(666)\n",
    "w  = 1e-4*np.random.random(n*K) \n",
    "# w: a (7840, ) array a small, random initial guess\n",
    "# during computation it will be resized to total 10 columns standing for 10 sets of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.exp(np.matmul(X,w.reshape(n,K)))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s / np.apply_along_axis(np.sum, axis=1, arr= s).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model sigma(x; w), \n",
    "# w: 10 sets weights\n",
    "# x: training samples, ofshape 60000 row by 784\n",
    "def sigma(w,X):\n",
    "    w = w.reshape(n,K)\n",
    "    s = np.exp(np.matmul(X,w))\n",
    "    total = np.apply_along_axis(np.sum, axis=1, arr=s).reshape(-1,1)\n",
    "    prob = s / total\n",
    "    return prob\n",
    "\n",
    "# loss function, modulo by N (size of training data)\n",
    "# a vectorized implementation with a for loop with only 10 iterations\n",
    "def loss(w,X,y):\n",
    "    loss_components = np.zeros(N)\n",
    "    for k in range(K):\n",
    "        loss_components += np.log(sigma(w,X))[:,k] * (y == k)\n",
    "    # above is a dimension (60000,) array\n",
    "    return -np.mean(loss_components) # same with loss_components.sum()/N\n",
    "\n",
    "def gradient_loss(w,X,y):\n",
    "    gradient_for_each_weight_class = np.empty([n,K]) \n",
    "    # 10 columns, each column represent a graident\n",
    "    for k in range(K):\n",
    "        gradient_for_all_training_data_for_class_k = (sigma(w,X)[:,k] - (y==k)).reshape(-1,1)*X\n",
    "        gradient_for_each_weight_class[:,k] = np.mean(gradient_for_all_training_data_for_class_k, axis=0)\n",
    "    # we should return a (784,) array, which is averaging all 60000 training data\n",
    "    return gradient_for_each_weight_class.reshape(n*K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_for_each_weight_class = np.empty([n,K]) # same shape with reshaped w\n",
    "for k in range(10):\n",
    "    gradient_for_all_training_data_for_class_k = (sigma(w,X)[:,k] - (y==k)).reshape(-1,1)*X\n",
    "    gradient_for_each_weight_class[:,k] = np.mean(gradient_for_all_training_data_for_class_k, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_residual = (sigma(w,X)[:,k] - (y==k)).reshape(-1,1)\n",
    "label_residual_avg = np.mean(label_residual*X, axis = 0)\n",
    "label_residual_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(w,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(gradient_loss(w,X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation function\n",
    "For a fixed set of weights `sigma(w)` gives 10 probabilities for each sample (training or testing), here we want to implement a cross-validation function, takes input of `X_train` or `X_test`, compute the class label of the highest probability for each samples, and returns the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a cross validating function computing \n",
    "def cross_validate_acc(w,X,y):\n",
    "    prob = sigma(w,X) # for each sample, it computes 10 probabilities based on current weight w\n",
    "    highest_prob_index = np.argmax(prob, axis=1)\n",
    "    return np.mean(highest_prob_index == y.astype(int))\n",
    "    # same with (highest_prob_index == y.astype(int)).sum()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = sigma(w,X)\n",
    "prob[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(prob, axis=1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-5  # step size (learning rate)\n",
    "num_steps = 5\n",
    "\n",
    "loss_at_eachstep = np.zeros(num_steps) # record the change of the loss function\n",
    "for i in range(num_steps):\n",
    "    loss_at_eachstep[i] = loss(w,X,y)\n",
    "    dw = gradient_loss(w,X,y)\n",
    "    w = w - eta * dw\n",
    "    print(\"loss after\", i+1, \"iterations is: \", loss(w,X,y))\n",
    "    print(\"Training accuracy after\", i+1, \"iterations is: \", cross_validate_acc(w,X,y))\n",
    "    print(\"Testing accuracy after\", i+1, \"iterations is: \", cross_validate_acc(w,X_test,y_test))\n",
    "    # keep track of training accuracy just making sure we are in the right direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slow, slow, slow\n",
    "Because our dataset is big. One iteration of the gradient descent requires evaluating the gradient for all the training samples, and it takes takes $O(N\\cdot d)$ cpu time ($N$: number of training samples, $d$:number of features in each sample). Stochastic gradient descent to remedy next time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always scikit-learn to rescue\n",
    "\n",
    "We can use `scikit-learn`'s [`LogisticRegression()` class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) in the `linear_model` to perform this task for us. Quoting the reference:\n",
    "> In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the 'multi_class' option is set to 'ovr', and uses the cross- entropy loss if the 'multi_class' option is set to 'multinomial'. (Currently the 'multinomial' option is supported only by the 'lbfgs', 'sag' and 'newton-cg' solvers.)\n",
    "\n",
    "> For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones.\n",
    "For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; 'liblinear' is limited to one-versus-rest schemes.\n",
    "'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas 'liblinear' and 'saga' handle L1 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the example\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "mnist_clf = LogisticRegression(random_state=666, \n",
    "                               solver='lbfgs', tol= 1e-6, max_iter = 1e3, verbose=True, \n",
    "                               multi_class='multinomial')\n",
    "# a faster solver is sag according to the reference\n",
    "# verbose is printing output during training (only applies to lbfgs as solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_clf.fit(X_train[:10000,:], y_train[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_clf.predict(X_test[:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_clf.score(X_test[:500,:], y_test[:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
