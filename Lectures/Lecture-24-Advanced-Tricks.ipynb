{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 24: Machine learning related packages\n",
    "\n",
    "In this lecture, we will some packages and functions. \n",
    "Main topic: `KFold`.\n",
    "\n",
    "Optional: `autograd` and `pytorch`, through some simple examples. `scitime` that can be helpful in training neural network with multiple hidden layers (>= 3), and/or large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=2)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "print(kf)  \n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional packages to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Neither of `autograd` and `pytorch` is installed in a typical Anaconda installation. Here is an instruction on how to install it on MacOS and Windows (the desktop in the MSTB/ALP labs).\n",
    "\n",
    "### Autograd\n",
    "For both MacOS and Windows system, please run the following command at Anaconda prompt (Windows) or command line (Linux/BSD-based system like MacOS)\n",
    "```\n",
    "conda install -c conda-forge autograd\n",
    "```\n",
    "\n",
    "### PyTorch\n",
    "For MacOS (not supporting CUDA), please run the following command:\n",
    "```\n",
    "conda install pytorch torchvision -c pytorch\n",
    "```\n",
    "For Windows, and to install the cpu-only version\n",
    "```\n",
    "conda install pytorch-cpu torchvision-cpu -c pytorch\n",
    "```\n",
    "If you have an NVIDIA GPU on Windows or Linux, you can install the CUDA-enabled version as well:\n",
    "```\n",
    "conda install pytorch torchvision cudatoolkit=x.0 -c pytorch\n",
    "```\n",
    "Change `x` to `10` or `9` depending on your CUDA toolkit version. You can check your CUDA toolkit's version directly on Windows by checking [your driver's version](https://docs.nvidia.com/deploy/cuda-compatibility/index.html), while on Linux, you can check by `nvcc --version` if CUDA is installed with PATH properly configed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "The `autograd` package provides automatic differentiation.\n",
    "\n",
    "Instruction of installation: \n",
    "\n",
    "Reference: \n",
    "* [Autograd: Effortless Gradients in Numpy](https://indico.lal.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf)\n",
    "* [Autograd Tutorial](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/tutorials/tut4.pdf)\n",
    "\n",
    "If you are interested in how `autograd` is implemented, you can refer to [\"Autodidact: a pedagogical implementation of Autograd\"](https://github.com/mattjj/autodidact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_sine(x):  # Taylor expansion to sine function\n",
    "    ans = x\n",
    "    currenterm = x\n",
    "    i = 0\n",
    "    while np.abs(currenterm) > 1e-6: # cut off when residual term is < 1e-6\n",
    "        currenterm = -currenterm * x**2 / ((2*i + 3) * (2*i + 2))\n",
    "        ans += currenterm\n",
    "        i += 1\n",
    "    return ans\n",
    "\n",
    "grad_sine = grad(taylor_sine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grad_sine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "grad_sine(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise for yourself to try\n",
    "Recall that we have tried to compute the gradient of Beale function by hand (refer to Lab 6 practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import value_and_grad\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def beale(x):\n",
    "    term1 = (1.5 - x[0] + x[0]*x[1])**2 \n",
    "    term2 = (2.25 - x[0] + x[0]*x[1]**2)**2 \n",
    "    term3 = (2.625 - x[0] + x[0]*x[1]**3)**2\n",
    "    return term1+term2+term3\n",
    "\n",
    "# Build a function that also returns gradients using autograd.\n",
    "beale_with_grad = value_and_grad(beale)\n",
    "\n",
    "# Optimize using conjugate gradients.\n",
    "result = minimize(beale_with_grad, x0=np.array([3.0, 2.0]), jac=True, method='CG')\n",
    "print(\"The minimum is achieved at\", result.x, \"with value\", result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beale_with_grad(np.array([0, 2.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "In the following few cells, we are gonna replicate what we have done the last two lectures in an open-source platform PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let us see a plain and simple numpy neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a minimal example of least-square loss (no activation on the last layer) on a neural net\n",
    "import numpy as np\n",
    "\n",
    "# N is the sample size (or current mini-batch size); \n",
    "# D_in is input dimension;\n",
    "# N_H is hidden dimension; \n",
    "# D_out is output dimension.\n",
    "N, D_in, N_H, D_out = 64, 1000, 100, 3\n",
    "\n",
    "# Create random input and output data\n",
    "# np.random.seed(666)\n",
    "X = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights, no bias since X, y are centered at 0\n",
    "# w1: weights for input layer -> hidden layer\n",
    "# w2: weights for hidden layer -> output layer\n",
    "w1 = np.random.randn(D_in, N_H)\n",
    "w2 = np.random.randn(N_H, D_out)\n",
    "\n",
    "# step size/learning rate\n",
    "eta = 1e-6\n",
    "for m in range(1000):\n",
    "    # Forward pass: compute predicted y\n",
    "    z = np.matmul(X,w1)\n",
    "    a = z*(z>0) # relu\n",
    "    y_pred = np.matmul(a,w2)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = np.matmul(a.T, grad_y_pred)\n",
    "    grad_a = np.matmul(grad_y_pred, w2.T)\n",
    "    grad_w1 = np.matmul(X.T, grad_a*(z>0))\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= eta * grad_w1\n",
    "    w2 -= eta * grad_w2\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.sum((y_pred - y)**2)\n",
    "    if m % 100 == 0:\n",
    "        print(\"LS loss after\", m+1, \n",
    "              \"iterations is\",loss, \n",
    "              \"with a training R squared\", 1 - loss/(np.sum((y- np.mean(y))**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us look at the PyTorch version\n",
    "\n",
    "The following example is slightly adapted from the PyTorch tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is the sample size (or current mini-batch size); \n",
    "# D_in is input dimension;\n",
    "# N_H is hidden dimension; \n",
    "# D_out is output dimension.\n",
    "N, D_in, N_H, D_out = 64, 1000, 100, 3\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, N_H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(N_H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for m in range(1000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    # torch.mm() is the matrix multiplication in PyTorch\n",
    "    # https://pytorch.org/docs/0.4.0/torch.html#torch.mm\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if m % 100 == 0:\n",
    "        print(\"LS loss after\", m, \n",
    "              \"iterations is\",loss.item(), \n",
    "              \"with a training R squared\", 1 - (loss.item())/((y- y.mean()).pow(2).sum().item()))\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scitime\n",
    "\n",
    "Training time estimation for `scikit-learn` algorithms. Currently supporting: \n",
    "* [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "* [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "* [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "You can try support vector machine (SVM) based classification and/or random forest based classifier for the final project as well.\n",
    "\n",
    "To install `scitime`, you can do\n",
    "```\n",
    "conda install -c conda-forge scitime\n",
    "```\n",
    "An example of `KMeans` running time is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from scitime import Estimator\n",
    "\n",
    "# example for kmeans clustering\n",
    "estimator = Estimator(meta_algo='RF', verbose=3)\n",
    "km = KMeans()\n",
    "\n",
    "X = np.random.rand(100000,10)\n",
    "# run the estimation\n",
    "estimation, lower_bound, upper_bound = estimator.time(km, X)\n",
    "\n",
    "# compare to the actual training time\n",
    "start_time = time.time()\n",
    "km.fit(X)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"elapsed time:\", elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
