{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 20: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lecture we have learned how to set up and use the softmax regression to classify all 10 handwritten digits based on pixel intensities on a 28x28 grid (MNIST dataset).\n",
    "\n",
    "However, the training is slow on a decent configured laptop due to the size the dataset and the size of the parameter.\n",
    "\n",
    "Today we will learn a new method called Stochastic Gradient Descent (SGD), one of the two pillars of the deep learning (the other being backpropagation).\n",
    "\n",
    "> Every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize (stochastic) gradient descent.\n",
    "\n",
    "References:\n",
    "* [Why Momentum Really Works](https://distill.pub/2017/momentum/)\n",
    "* [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla GD \n",
    "We consider a loss function (e.g., the one we saw in softmax regression) which is the average of the sample-wise loss:\n",
    "\n",
    "$$L(\\mathbf{w}) := L(\\mathbf{w}; X,\\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N f_i(\\mathbf{w}; \\mathbf{x}^{(i)},y^{(i)})$$ \n",
    "\n",
    "which has the weights $\\mathbf{w}$ as the parameters. Let me copy the softmax loss function here for comparison:\n",
    "\n",
    "$$\n",
    "L (\\mathbf{w})  = - \\frac{1}{N}\\sum_{i=1}^N \\left\\{\\sum_{k=1}^K\n",
    "1_{\\{y^{(i)} = k\\}} \\ln \\Bigg( \\frac{\\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})}{\\sum_{j=1}^{K} \n",
    "\\exp\\big(\\mathbf{w}_j^{\\top} \\mathbf{x}^{(i)} \\big) }  \\Bigg)\\right\\}.\n",
    "$$\n",
    "\n",
    "Then the gradient descent method for it reads:\n",
    "\n",
    "> Choose initial guess $\\mathbf{w}_0$, step size (learning rate) $\\eta$, number of iterations $M$<br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{k+1} =  \\mathbf{w}_k - \\eta\\nabla_{\\mathbf{w}} L(\\mathbf{w}_k) =  \\mathbf{w}_k - \\frac{\\eta}{N}\\sum_{i=1}^N \\nabla f_i(\\mathbf{w}; \\mathbf{x}^{(i)},y^{(i)})$\n",
    "\n",
    "The gradient has to be evaluated $N$ times in one iteration, each evaluation involves a matrix matrix multiplication of order $O(n)$ (number of features in one sample).\n",
    "\n",
    "### Drawbacks: \n",
    "* Large amount of gradient evaluation, long computation time, wasting electricity, etc. \n",
    "* Suppose we add more examples to our training set. For simplicity, imagine we just add an extra copy of every training example (but the computer algorithm does not know it is the same samples): then the amount of work doubles!\n",
    "$$\n",
    "\\nabla L = \\frac{1}{2N}\\sum_{i=1}^N \\nabla f_i(\\mathbf{w}) + \\frac{1}{2N}\\sum_{i=1}^N \\nabla f_i(\\mathbf{w}) ,\n",
    "$$\n",
    "even for the same loss function.\n",
    "* The training examples arrive one-at-a-time (or several-at-a-time) as the model is learning (gradient descent). Should we include these into the original dataset and re-compute the gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Suppose our loss function is still:\n",
    "\n",
    "$$L := L(\\mathbf{w}; X,\\mathbf{y}) =  \\frac{1}{N}\\sum_{i=1}^N f_i(\\mathbf{w}; \\mathbf{x}^{(i)},y^{(i)}),$$\n",
    "\n",
    "where $X = (\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)})^{\\top}$ are the training samples, $\\mathbf{y} = (y^{(1)}, \\dots, y^{(N)})^{\\top}$ are the labels for the training samples.\n",
    "\n",
    "> Choose initial guess $\\mathbf{w}_0$, step size (learning rate) $\\eta$, number of inner iterations $M$, number of epochs $n_E$ <br><br>\n",
    ">    Set $\\mathbf{w}_{M+1} = \\mathbf{w}_0$ for epoch $e=0$<br>\n",
    ">    For epoch $n=1,2, \\cdots, n_E$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; $\\mathbf{w}_{0}$ for the current epoch is $\\mathbf{w}_{M+1}$ for the previous epoch.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; Randomly shuffle the training samples.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; For $m=0,1,2, \\cdots, M$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{m+1} = \\mathbf{w}_m - \\eta \\nabla f_i(\\mathbf{w}; \\mathbf{x}^{(m)},y^{(m)})$\n",
    "\n",
    "If $M = N$, which is the current batch of all training samples, one outer iteration is called a completed *epoch*.\n",
    "\n",
    "### Vanilla SGD: Single gradient evaluation at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Let us give it a go on the linear regression. This time, we are using `scikit-learn`'s built-in dataset. For the linear regression, we can use `scikit-learn`'s `LinearRegression()` class for the multivariate regression as Lecture 17, but since we are illustrating SGD, we are implementing ourselves. Recall the loss function with the $L^2$ regularization and the gradient: let the weight $\\mathbf{w} = (w_0, \\widehat{\\mathbf{w}})$ where $w_0$ is the bias, and $\\widehat{\\mathbf{w}}$ is the vector containing the weights for the features of the dataset\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N  \n",
    "\\left( [1, \\;\\mathbf{x}^{(i)}]^{\\top} \\mathbf{w} - y^{(i)} \\right)^2 \n",
    "+ \\epsilon |\\widehat{\\mathbf{w}}|^2,\n",
    "\\\\\n",
    "\\frac{\\partial L(w)}{\\partial \\mathbf{w}} = \\frac{2}{N}\\sum_{i=1}^N [1, \\;\\mathbf{x}^{(i)}]\\left( [1, \\;\\mathbf{x}^{(i)}]^{\\top} \\mathbf{w} - y^{(i)}\\right) + 2\\epsilon\\, [0, \\widehat{\\mathbf{w}}]\n",
    "$$\n",
    "\n",
    "Reference: [Scikit-learn's dataset loading utilities](https://scikit-learn.org/stable/datasets/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# let us load the house price dataset \n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# pre-process the data to have an artificial column of ones\n",
    "X_train = np.concatenate((np.ones([len(y_train),1]), X_train), axis=1) \n",
    "# concatenating ones to the X_train as the first column (along axis 1)\n",
    "X_test = np.concatenate((np.ones([len(y_test),1]), X_test), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1e-2*np.random.normal(size=np.shape(X_train)[1]) # weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "X_test[:4,:].dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-3 # regularization parameter\n",
    "\n",
    "# our model, returns the linear function [1 X]^T w\n",
    "def f(w, X):\n",
    "    # X is the training data\n",
    "    return np.matmul(X,w)\n",
    "\n",
    "# loss function = total square error on the given data set X,y\n",
    "def loss(w, X, y):\n",
    "    # N = len(y) \n",
    "    # this is one of the key, if y is only part of/one of the data label, then N will be small!\n",
    "    residual_components = f(w, X) - y\n",
    "    regularization = eps*np.sum(w[1:]**2)\n",
    "#     return (1/len(y))*np.sum(residual_components**2) + regularization\n",
    "    return np.mean(residual_components**2) + regularization\n",
    "    # the second implementation is prefered due to the len(y) may not exist\n",
    "\n",
    "def gradient_loss(w, X, y):\n",
    "    # An alternative implementation\n",
    "    #     N = len(y)\n",
    "    #     loss_components = f(w, X) - y\n",
    "    #     gradient = loss_components.reshape(-1,1)*X\n",
    "    #     return (2/N)*np.sum(gradient, axis=0)\n",
    "    gradient_for_all_training_data = (f(w,X) - y).reshape(-1,1)*X\n",
    "    gradient_for_regularization = 2*eps*w\n",
    "    gradient_for_regularization[0] = 0 # no gradient for the bias in the regularization term\n",
    "    gradient_mean_training_data = np.mean(gradient_for_all_training_data, axis=0)\n",
    "    # we should return a (14,) array, which is averaging all training data\n",
    "    return 2*gradient_mean_training_data + gradient_for_regularization\n",
    "\n",
    "# we define a cross validating function to compute the R^2 score \n",
    "def cross_validate_rsquared(w, X, y):\n",
    "    y_pred = f(w, X)\n",
    "    return 1 - (np.sum((y- y_pred)**2))/(np.sum((y- y.mean())**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(gradient_loss(w,X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_loss(w,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(w * X_train, axis=1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.matmul(X_train,w)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let us try gradient descent \n",
    "#### just making sure our implementation above is good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta = 1e-6# step size (learning rate)\n",
    "num_steps = 2000\n",
    "w = np.array([30.24675099,  -0.11305592,   0.03011046,   0.04038072, 2.7844382 , -17.20263339,   4.4388352 ,  \n",
    "              -0.00629636,  -1.44786537,   0.26242974,  -0.01064679,  -0.91545624, 0.01235133,  -0.50857142]) \n",
    "w += 2e-1*np.random.normal(size=np.shape(X_train)[1])  \n",
    "\n",
    "loss_at_eachstep = np.zeros(num_steps) # record the change of the loss function\n",
    "for i in range(num_steps):\n",
    "    loss_at_eachstep[i] = loss(w,X_train,y_train)\n",
    "    dw = gradient_loss(w,X_train,y_train)\n",
    "    w = w - eta * dw\n",
    "    if i % 200 == 0:\n",
    "        print(\"loss after\", i+1, \"iterations is: \", loss(w,X_train,y_train))\n",
    "        print(\"Training R squared after\", i+1, \"iterations is: \", cross_validate_rsquared(w, X_train, y_train))\n",
    "        print(\"Test R squared after\", i+1, \"iterations is: \", cross_validate_rsquared(w, X_test, y_test))\n",
    "    # keep track of training accuracy just making sure we are in the right direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_steps), loss_at_eachstep)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn's version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "housing_reg = linear_model.LinearRegression()\n",
    "housing_reg.fit(X_train, y_train)\n",
    "print(\"Training R squared is:\", housing_reg.score(X_train,y_train))\n",
    "print(\"Test R squared is:\", housing_reg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing if our functions are implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = housing_reg.coef_\n",
    "w[0] = housing_reg.intercept_\n",
    "loss(w,X_test,y_test)\n",
    "gradient_loss(w,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-6# step size (learning rate)\n",
    "num_epochs = 300\n",
    "M = len(y_train)\n",
    "# N = 200\n",
    "w = np.array([30.24675099,  -0.11305592,   0.03011046,   0.04038072, 2.7844382 , -17.20263339,   4.4388352 ,  \n",
    "              -0.00629636,  -1.44786537,   0.26242974,  -0.01064679,  -0.91545624, 0.01235133,  -0.50857142]) \n",
    "w += 2e-1*np.random.normal(size=np.shape(X_train)[1])  \n",
    "\n",
    "sgd_loss_at_eachstep = np.zeros([num_epochs,N]) # record the change of the loss function\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    shuffle_index = np.random.permutation(N)\n",
    "    for m in range(M):\n",
    "        i = shuffle_index[m]\n",
    "        sgd_loss_at_eachstep[e,m] = loss(w,X_train,y_train)\n",
    "        dw = gradient_loss(w,X_train[i,:],y_train[i])\n",
    "        w = w - eta * dw\n",
    "        if m % 100 ==0 and e % 100 == 0:\n",
    "            print(\"loss after\", e+1, \"epochs and \", m+1, \"iterations is: \", loss(w,X_train,y_train))\n",
    "            print(\"Training R squared after\", e+1, \"epochs and \", m+1, \n",
    "                  \"iterations is:\", cross_validate_rsquared(w, X_train, y_train))\n",
    "            print(\"Testing R squared after\", e+1, \"epochs and \", m+1, \n",
    "                  \"iterations is:\", cross_validate_rsquared(w, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plot = 500\n",
    "plt.plot(range(5,num_plot), sgd_loss_at_eachstep.reshape(-1)[5:num_plot], label=\"SGD loss\")\n",
    "plt.plot(range(5,num_plot), loss_at_eachstep[5:num_plot], linewidth=2, label= \"GD loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch SGD\n",
    "\n",
    "In the vanilla SGD, each parameter $\\mathbf{w}$ update is computed w.r.t one training sample randomly selected. In mini-batch SGD, the update is computed for a mini-batch (a small number of training samples), as opposed to a single example. The reason for this is twofold: \n",
    "* This reduces the variance in the parameter update and can lead to more stable convergence.\n",
    "* This allows the computation to be more efficient, since our code is written in a vectorized way. \n",
    "\n",
    "A typical mini-batch size is $2^k$ (32, 256, etc), although the optimal size of the mini-batch can vary for different applications, and size of dataset (e.g., AlphaGo training uses mini-batch size of 2048 positions).\n",
    "\n",
    "> Choose initial guess $\\mathbf{w}_0$, step size (learning rate) $\\eta$, <br>\n",
    "batch size $n_B$, number of inner iterations $M\\leq N/n_B$, number of epochs $n_E$ <br><br>\n",
    ">    Set $\\mathbf{w}_{M+1} = \\mathbf{w}_0$ for epoch $e=0$<br>\n",
    ">    For epoch $n=1,2, \\cdots, n_E$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; $\\mathbf{w}_{0}$ for the current epoch is $\\mathbf{w}_{M+1}$ for the previous epoch.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; Randomly shuffle the training samples.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; For $m=0,1,2, \\cdots, M$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{m+1} = \\mathbf{w}_m -  \\frac{\\eta}{n_B}\\sum_{i=1}^{n_B} \\nabla f_i(\\mathbf{w}; \\mathbf{x}^{(m+i)},y^{(m+i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-6# step size (learning rate)\n",
    "num_epochs = 500\n",
    "n_B = 16 # number of mini-batch\n",
    "M = int(len(y_train)/n_B)\n",
    "w = np.array([30.24675099,  -0.11305592,   0.03011046,   0.04038072, 2.7844382 , -17.20263339,   4.4388352 ,  \n",
    "              -0.00629636,  -1.44786537,   0.26242974,  -0.01064679,  -0.91545624, 0.01235133,  -0.50857142]) \n",
    "w += 2e-1*np.random.normal(size=np.shape(X_train)[1])  \n",
    "\n",
    "sgdmini_loss_at_eachstep = np.zeros([num_epochs,M]) # record the change of the loss function\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    shuffle_index = np.random.permutation(N)\n",
    "    for m in range(M):\n",
    "        i = shuffle_index[m:m+n_B]\n",
    "        sgdmini_loss_at_eachstep[e,m] = loss(w,X_train,y_train)\n",
    "        dw = gradient_loss(w,X_train[i,:],y_train[i])\n",
    "        w = w - eta * dw\n",
    "        if m % 10 ==0 and e % 60 == 0:\n",
    "            print(\"loss after\", e+1, \"epochs and \", m+1, \"iterations is: \", loss(w,X_train,y_train))\n",
    "            print(\"Training R squared after\", e+1, \"epochs and \", m+1, \n",
    "                  \"iterations is:\", cross_validate_rsquared(w, X_train, y_train))\n",
    "            print(\"Testing R squared after\", e+1, \"epochs and \", m+1, \n",
    "                  \"iterations is:\", cross_validate_rsquared(w, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "num_plot = 500\n",
    "plt.figure(figsize= [16,8])\n",
    "plt.plot(range(5,num_plot), sgdmini_loss_at_eachstep.reshape(-1)[5:num_plot], 'b-*',\n",
    "         label=\"SGD mini-batch loss\")\n",
    "plt.plot(range(5,num_plot), sgd_loss_at_eachstep.reshape(-1)[5:num_plot], \n",
    "         label=\"SGD loss\", linewidth=1, color = 'green' )\n",
    "plt.plot(range(5,num_plot), loss_at_eachstep[5:num_plot], 'r',\n",
    "         linewidth=2, label= \"GD loss\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
