{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 Practice: \n",
    "\n",
    "This notebook does not need to be submitted. Problems similar to problem 1 will be featured in the midterm exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d # plotting surfaces\n",
    "from matplotlib.colors import LogNorm # for later use, display colormap in log scale\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function you can use to plot the convergence in contour using x_vals, y_vals\n",
    "def plot_gradient_descent(x_vals,y_vals,f):\n",
    "    X = np.linspace(-4.5,4.5,300)\n",
    "    Y = np.linspace(-4.5,4.5,300)\n",
    "    X, Y = np.meshgrid(X,Y)\n",
    "    Z = f(X, Y)\n",
    "\n",
    "    # these 4 lines of code is plotting the contour\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    CS = ax.contour(X, Y, Z, levels=np.logspace(0, 5, 35), cmap='jet', norm = LogNorm())\n",
    "    plt.axis('tight')\n",
    "    ax.clabel(CS, inline=True, fontsize=8)\n",
    "    \n",
    "    num_steps = len(x_vals)\n",
    "    delta_n = num_steps//10\n",
    "    for i in range(0,num_steps-delta_n,delta_n):\n",
    "        # plt.scatter(x_vals[i], y_vals[i])\n",
    "        plt.arrow(x_vals[i], y_vals[i], (x_vals[i+delta_n] - x_vals[i]), \n",
    "              (y_vals[i+delta_n] - y_vals[i]), \n",
    "              head_width=0.3, head_length=0.2, linewidth = 1.5, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent of Beale function\n",
    "Reference: Lecture 12 notebook\n",
    "\n",
    "In this lab we will continue to explore the gradient descent method for Beale function, which is one of the [benchmark for testing your optimization algorithm](https://en.wikipedia.org/wiki/Test_functions_for_optimization):\n",
    "$$\\displaystyle f(x,y)=\\left(1.5-x+xy\\right)^{2}+\\left(2.25-x+xy^{2}\\right)^{2} \n",
    "+\\left(2.625-x+xy^{3}\\right)^{2}$$\n",
    "We know that this function has the global minimum is achieved at $(3,0.5)$. But it has lots of traps (saddle points and local minima, very flat plateau which yields vanishing gradient near the global minimum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla gradient descent\n",
    "\n",
    "> Choose initial guess $(x_0,y_0)$ and step size (learning rate) $\\eta$<br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $(x_{k+1},y_{k+1}) =  (x_k,y_k) - \\eta\\nabla f(x_k,y_k) $\n",
    "\n",
    "A self-sustained implementation of the algorithm is as follows, where we use the *central difference* to approximate partial derivatives:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + h, y) - f(x - h,y )}{2h}$$\n",
    "and\n",
    "$$\\frac{\\partial f}{\\partial y} \\approx \\frac{f(x, y+h) - f(x,y-h)}{2h}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(f, x0 = (0,0), eta=1e-2, h=1e-6, num_steps=200):\n",
    "    '''\n",
    "    Gradient descent algorithm using the numerical gradient.\n",
    "    f: function to be minimized\n",
    "    x0: initial guess, array-like (tuple, list, array)\n",
    "    eta: step size\n",
    "    h: numerical gradient's h\n",
    "    num_steps: total number of iterations\n",
    "    '''\n",
    "    x, y = x0[0], x0[1]\n",
    "    \n",
    "    numpartialx = lambda x, y: 0.5*(f(x+h, y)-f(x-h, y))/h\n",
    "    numpartialy = lambda x, y: 0.5*(f(x, y+h)-f(x, y-h))/h\n",
    "\n",
    "    x_vals = np.zeros(num_steps)\n",
    "    y_vals = np.zeros(num_steps)\n",
    "    f_vals = np.zeros(num_steps)\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        dx, dy = numpartialx(x,y), numpartialy(x,y)\n",
    "        x -= eta*dx\n",
    "        y -= eta*dy\n",
    "        x_vals[i], y_vals[i], f_vals[i] = x, y, f(x,y)\n",
    "\n",
    "    return x_vals, y_vals, f_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Nesterov's accelerated gradient (NAG)\n",
    "Implement the following routine, by modifying the gradient descent above:\n",
    "> Choose $(x_0,y_0)$, $\\eta$, $\\gamma$, and let $\\mathbf{v}_{-1} = (0,0)$ <br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;   $\\mathbf{v}_k = \\gamma \\mathbf{v}_{k-1} \n",
    "+ \\eta \\nabla f\\big( (x_k,y_k) - \\gamma \\mathbf{v}_{k-1} \\big)$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $(x_{k+1},y_{k+1}) =  (x_k,y_k) - \\mathbf{v}_k  $\n",
    "\n",
    "$\\mathbf{v}_k$ can be viewed as an accelerated gradient. Notice that the gradient, instead of being evaluated at $(x_k,y_k)$, is evaluated at $(x_k,y_k) - \\gamma \\mathbf{v}_{k-1}$.\n",
    "\n",
    "This is called Nesterov's accelerated gradient by incoporating \"momentum\" and trying to extrapolating the gradient at the next iteration step.\n",
    "\n",
    "## Questions\n",
    "\n",
    "* Implement the NAG using the template above (vanilla gradient descent). Nesterov suggested in his original paper that a good candidate value for $\\gamma$ is 0.9, please use this as the default value for $\\gamma$ in the input argument.\n",
    "\n",
    "* After you define the function in the following cell, try $(x_0,y_0) = (1,2)$, step size $\\eta = 10^{-3}$, $\\gamma = 0.8$, and total $1000$ steps for NAG on the Beale function. \n",
    "\n",
    "* Try changing $h = 10^{-6}$ to $h=10^{-1}$ in the numerical partial derivatives, do you still get the same result?\n",
    "\n",
    "* Try $(x_0,y_0) = (-2,-2)$, step size $\\eta = 10^{-3}$, $\\gamma = 0.7$, and total $1000$ steps for the Beale function. Please keep $h = 10^{-6}$ in the numerical partial derivatives as given in default values for that argument. Do you get the same result?\n",
    "\n",
    "* Try using the vanilla gradient descent, using the same parameter setting (without the accelarated momentum correction) with the first questions ($(x_0,y_0) = (1,2)$), you should observe that the gradient descent converges not even near the point of the minimum point $(3,0.5)$. Moreover, the vanilla gradient descent is very sensitive to the step size for Beale function. Yet, with the NAG, we should end reasonably near it with a moderately big step size (try $\\eta = 5\\cdot 10^{-3}$).\n",
    "\n",
    "HINT: instead of letting gradient update at each iteration, you might wanna keep track the gradient at each iteration (or at least storing the gradient from the previous iteration), and treat the $0$-th iteration different from the others.\n",
    "\n",
    "Reference: [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 (optional read)\n",
    "Reading other person's implementation or commonly-used package's manual is a data-scientist's bread and butter.\n",
    "\n",
    "* Read the [Newton method for optimizing a function](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) which uses the information of the second derivatives (Hessian).\n",
    "\n",
    "* Read the [`scipy.optimize.minimize` manual](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) in `SciPy`, write a short script using this function to minimize Beale function with initial guess $(4,4)$ and `method` being Newton-CG. \n",
    "\n",
    "Note that in [Newton-CG](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html#optimize-minimize-newtoncg)'s manual, the parameter `jac` is required, basically this is the gradient function. In the reference's example, Rosenbrock function is used:\n",
    "```python\n",
    "from scipy.optimize import minimize, rosen, rosen_der\n",
    "rosen()\n",
    "```\n",
    "\n",
    "Checking the help of `rosen`  and `rosen_der` in the [Scipy optimize tutorial](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html), and mimic the example re-define the Beale using `rosen`'s format and its exact gradient using `rosen_der`'s format.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
