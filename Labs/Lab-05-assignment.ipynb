{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 Assignment: \n",
    "\n",
    "### Please type your name here:\n",
    "### Do not forget to change the filename with your name appended\n",
    "\n",
    "In this lab please fill the indicated cells with your code and explainations, ***run*** everything (select `cell` in the menu, and click `Run all`), save the notebook with your name appended to the filename (for example, `Lab-05-assignment-caos.ipynb`), and upload it to Canvas.\n",
    "<br><br>\n",
    "This lab assignment contains only 1 problem. \n",
    "<br><br>\n",
    "Read each problem carefully and answer them the best you can. You may copy the code from the Lecture 14's notebook. Even though a better way would be copy the code from Lecture 14, make the codes comment-like, type the codes by yourself using auto-completion.\n",
    "<br><br>\n",
    "For how to use a function, instead of asking others (TA, friend, your neighbor), you can put the cursor inside an empty parenthesis, press `Shift + Tab` (hold the shift key, press tab) to read the help in the pop up window, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might find the following useful\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d # plotting surfaces\n",
    "from matplotlib.colors import LogNorm # for later use, display colormap in log scale\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function you can use to plot the convergence in contour using x_vals, y_vals\n",
    "def plot_gradient_descent(x_vals,y_vals,f):\n",
    "    X = np.linspace(-4.5,4.5,300)\n",
    "    Y = np.linspace(-4.5,4.5,300)\n",
    "    X, Y = np.meshgrid(X,Y)\n",
    "    Z = f(X, Y)\n",
    "\n",
    "    # these 4 lines of code is plotting the contour\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    CS = ax.contour(X, Y, Z, levels=np.logspace(0, 5, 35), cmap='jet', norm = LogNorm())\n",
    "    plt.axis('tight')\n",
    "    ax.clabel(CS, inline=True, fontsize=8)\n",
    "    \n",
    "    num_steps = len(x_vals)\n",
    "    delta_n = num_steps//20\n",
    "    for i in range(0,num_steps-delta_n,delta_n):\n",
    "        # plt.scatter(x_vals[i], y_vals[i])\n",
    "        plt.arrow(x_vals[i], y_vals[i], (x_vals[i+delta_n] - x_vals[i]), \n",
    "              (y_vals[i+delta_n] - y_vals[i]), \n",
    "              head_width=0.3, head_length=0.2, linewidth = 1.5, color='red')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent of Beale function\n",
    "Reference: Lecture 12 notebook, Lab 5 practice.\n",
    "\n",
    "In this lab we will continue to explore the gradient descent method for Beale function, which is one of the [benchmark for testing your optimization algorithm](https://en.wikipedia.org/wiki/Test_functions_for_optimization):\n",
    "$$\\displaystyle f(x,y)=\\left(1.5-x+xy\\right)^{2}+\\left(2.25-x+xy^{2}\\right)^{2} \n",
    "+\\left(2.625-x+xy^{3}\\right)^{2}$$\n",
    "We know that this function has the global minimum is achieved at $(3,0.5)$. But it has lots of traps (saddle points and local minima, very flat gradient near the global minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla gradient descent\n",
    "\n",
    "> Choose initial guess $(x_0,y_0)$ and step size (learning rate) $\\eta$<br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $(x_{k+1},y_{k+1}) =  (x_k,y_k) - \\eta\\nabla f(x_k,y_k) $\n",
    "\n",
    "A self-sustained implementation of the algorithm is as follows, where we use the *central difference* to approximate partial derivatives:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + h, y) - f(x - h,y )}{2h}$$\n",
    "and\n",
    "$$\\frac{\\partial f}{\\partial y} \\approx \\frac{f(x, y+h) - f(x,y-h)}{2h}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(f, x0 = (0,0), eta=1e-2, h=1e-6, num_steps=200):\n",
    "    '''\n",
    "    Gradient descent algorithm using the numerical gradient.\n",
    "    f: function to be minimized\n",
    "    x0: initial guess, array-like (tuple, list, array)\n",
    "    eta: step size\n",
    "    h: numerical gradient's h\n",
    "    num_steps: total number of iterations\n",
    "    '''\n",
    "    x, y = x0[0], x0[1]\n",
    "    \n",
    "    numpartialx = lambda x, y: 0.5*(f(x+h, y)-f(x-h, y))/h\n",
    "    numpartialy = lambda x, y: 0.5*(f(x, y+h)-f(x, y-h))/h\n",
    "\n",
    "    x_vals = np.zeros(num_steps)\n",
    "    y_vals = np.zeros(num_steps)\n",
    "    f_vals = np.zeros(num_steps)\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        dx, dy = numpartialx(x,y), numpartialy(x,y)\n",
    "        x -= eta*dx\n",
    "        y -= eta*dy\n",
    "        x_vals[i], y_vals[i], f_vals[i] = x, y, f(x,y)\n",
    "\n",
    "    return x_vals, y_vals, f_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Adaptive step-size (learning rate)\n",
    "Another popular gradient descent algorithm in the machine learning community is [Root Mean Square Prop or RMSprop](http://ruder.io/optimizing-gradient-descent/index.html#rmsprop), which changes the step size adaptively according to the magnitude of the gradient. This algorithm is invented by Geoff Hinton. Despite its vast popularity in the machine learning community, it is unpublished, and it was proposed firstly in his Coursera lecture notes.\n",
    "\n",
    "RMSprop is proposed to solve two problems in the vanilla gradient descent (GD):\n",
    "* Vanishing gradient: it is observed that in Lab 5 practice even after 2000 steps, even starting somewhere like $(1,2)$ reasonably near the true global minimum point $(3,0.5)$, the vanialla GD stops or \"plateaus\" on the plateau-like region near $(3,0.5)$ (the contour looks like a plateau). It is because the gradient is too small to drive the GD forward.\n",
    "* Step-size (learning rate) has to be small to be convergent: in Lab 5 practice, we also observe that, if starting from somewhere like $(4,4)$, a reasonably small step-size like $10^{-4}$ will make GD blow up fairly fast.\n",
    "\n",
    "The RMSprop algorithm is as follows:\n",
    "> Choose $(x_0,y_0)$, $\\eta$, $\\gamma$, $\\epsilon$, and let $s_{-1} = 1$ <br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;  $s_{k} = \\gamma s_{k-1} + (1 - \\gamma)\\, \\left|\\nabla f(x_k,y_k)\\right|^2$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle(x_{k+1},y_{k+1}) =  (x_k,y_k) -  \\frac{\\eta} {\\sqrt{s_{k}+ \\epsilon}} \\nabla f(x_k,y_k)$  \n",
    "\n",
    "where $\\left|\\nabla f(x_k,y_k)\\right|$ denotes the magnitude of the gradient vector.\n",
    "\n",
    "Normally the parameters are chosen as: $\\gamma = 0.9$, $\\epsilon = 10^{-3}$.\n",
    "\n",
    "*  Using $h = 10^{-6}$ in the numerical partial derivatives as in the `grad_descent` function, implement RMSprop as a function. Try $(x_0,y_0) = (1,2)$, step size $\\eta = 10^{-2}$, $\\gamma = 0.9$, $\\epsilon = 10^{-3}$, and total $1000$ steps for RMSprop on the Beale function. The correctly implemented algorithm is expected to converge reasonably close to the global minimum $(3.0, 0.5)$ (distance is less than $10^{-2}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop():\n",
    "    # your code here to replace pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "x_vals, y_vals, f_vals = rmsprop(f, x0 = (1,2), eta=1e-2, h=1e-6, eps = 1e-3, num_steps = num_steps)\n",
    "print(\"The value of f(x,y): \", f(x_vals[-1],y_vals[-1]), \"after\", \n",
    "      num_steps, \"iterations at point\", (x_vals[-1],y_vals[-1]))\n",
    "# let's see what the f(x,y) values were    \n",
    "\n",
    "plt.title(\"f(x,y) over gradient descent steps\")\n",
    "plt.plot(range(num_steps), f_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent(x_vals,y_vals,f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
