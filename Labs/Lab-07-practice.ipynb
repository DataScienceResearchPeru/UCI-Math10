{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Practice Week 7\n",
    "\n",
    "This notebook does not need to be submitted. This is only for you to gain experience and get some practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Multi-dimensional data fitting and gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem uses an artificial dataset generated by the [`make_regression` function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) in the `scikit-learn`'s dataset submodule. This time we have two features in each training sample. Recall in Week 6's lectures, we have learned the regressions of the citric acid (feature) vs fixed acidity (target) and volatile acidity (feature) vs fixed acidity (target). In both examples, the key problem is essentially a 1-dimensional problem since we using a line (1-d object) to \"fit\" the general trend. \n",
    "\n",
    "If we have two features, say a data point is $\\mathbf{x} = (x_1, x_2)$, and the label is $y$, then the linear regression is to find a linear function \n",
    "$$ h(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2 = \\mathbf{w}^{\\top} [1, \\mathbf{x}]$$\n",
    "\n",
    "such that $h(\\mathbf{x}) \\approx y$. Suppose we totally have $N$ training samples, our loss function can be written as:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}) = \n",
    "\\frac{1}{N}\\sum_{i=1}^N  \\left( [1, \\;\\mathbf{x}^{(i)}]^{\\top} \\mathbf{w} - y^{(i)} \\right)^2\n",
    "= \\frac{1}{N}\\sum_{i=1}^N  \n",
    "\\left( w_0 x_0^{(i)} + w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right)^2,\n",
    "$$\n",
    "where $x_0^{(i)} = 1$ is articially added to each training samples as the 0-th feature.\n",
    "\n",
    "Taking gradient for $w_0$, $w_1$, and $w_2$: for $k=0,1,2$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{2}{N}\\sum_{i=1}^N x^{(i)}_k \\left(h(\\mathbf{x}^{(i)}) - y^{(i)}\\right)\n",
    "= \\frac{2}{N}\\sum_{i=1}^N x^{(i)}_k \\left(w_0 x_0^{(i)} + w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right),\n",
    "$$\n",
    "which is the sum of the product of $k$-th feature and the residual $h(\\mathbf{x}^{(i)}) - y^{(i)}$. An easier expression for vectorization is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = \n",
    "= \\frac{2}{N}\\sum_{i=1}^N \\mathbf{x}^{(i)} \\left(w_0 x_0^{(i)} + w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero bias dataset\n",
    "\n",
    "If we already know that our dataset has zero bias, i.e., the best weight possible $w_0 = 0$, then our model simplifies to:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N  \\left(w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right)^2,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{2}{N}\\sum_{i=1}^N\\mathbf{x}^{(i)} \n",
    "\\left(w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# generate a zero bias dataset\n",
    "X, y = make_regression(n_samples=5000, n_features= 2, bias = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape # training sample's shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape # testing target values/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn for better visualization than pyplot\n",
    "# visualization of all data\n",
    "import seaborn as sns\n",
    "sns.set(style='white')\n",
    "_, ax = plt.subplots(figsize=[12,8])\n",
    "sns.scatterplot(ax = ax, x = X[:,0], y = X[:,1],  s = y, \n",
    "                # s is the size of the dot, we use our target value y as the size of the dot\n",
    "                alpha=0.4, edgecolors='w')\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: GD\n",
    "\n",
    "Adapt the gradient descent method from Lecture 16 to this case using the exact gradient. Implement the exact gradient (not numerical gradient) of the loss function in the cell below. When you have done training the model, compute the predicted target value using your model, then visualize the trained model's prediction using the cell which follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def h(w, X): \n",
    "    return np.matmul(X,w)\n",
    "\n",
    "# loss\n",
    "def loss(w, X, y):\n",
    "    residual_components = h(w, X) - y\n",
    "    return np.mean(residual_components**2)\n",
    "\n",
    "# gradient of the loss\n",
    "def gradient_loss(w, X, y):\n",
    "    # implemente the gradient here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your implementation of gradient descent here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the y_pred using your model\n",
    "y_pred = h(w, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(16, 8))\n",
    "sns.scatterplot(ax = axes[0], x = X_test[:,0], y = X_test[:,1],  s = y_pred, \n",
    "                alpha=0.4, color='b').set_title(\"Predicted value\", fontsize = 20)\n",
    "sns.scatterplot(ax = axes[1], x = X_test[:,0], y = X_test[:,1],  s = y_test, \n",
    "                alpha=0.4, color='r').set_title(\"Actual value\", fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Gradient checking\n",
    "\n",
    "So far we have worked with relatively straight-forward loss functions, and their gradients can be derived with pen-and-paper, and then be implemented directly. \n",
    "\n",
    "We also have used a forward difference to approximate gradient function.\n",
    "\n",
    "For more complex models that we will see later, for example the backpropagation for neural networks involving multiple nonlinear composite functions' gradient. The gradient computation can be notoriously difficult to debug and get right. Sometimes a subtle buggy implementation will manage to learn something that can look surprisingly reasonable (while performing less well than a correct implementation). Thus, even with a buggy implementation, it may not at all be apparent that anything is amiss. \n",
    "\n",
    "In this problem, we simply check if the gradient we implemented agrees with the numerical gradient (usually the central difference) up to certain tolerance we set. This is called *gradient checking*. Carrying out the derivative checking procedure described here will significantly increase your confidence in the correctness of your code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a function $g_k(\\mathbf{w})$ that purportedly computes $\\partial L/\\partial w_k$; we would like to check if $g_k(\\mathbf{w})$ is outputting correct derivative values. Let \n",
    "\n",
    "$$\\mathbf{e}_k = (0,\\dots, 1, \\dots, 0)^{\\top},$$\n",
    "\n",
    "be the unit vector in the $k$-th component. Then the gradient checking reads:\n",
    "\n",
    ">  $\\mathbf{w}$: the weights, $\\epsilon$: certain close to 0 values, $\\texttt{tol}$: tolerance we set<br>\n",
    "> If $\\displaystyle \\left| g_k(\\mathbf{w}) - \\frac{L(\\mathbf{w} + \\epsilon\\mathbf{e}_k) - L(\\mathbf{w} - \\epsilon\\mathbf{e}_k)}{2\\epsilon}\\right| < \\texttt{tol}$ for every $k$ <br><br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; Proceed to do gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "Add the gradient checking algorithm to your gradient descent code, you can copy the numerical gradient implementation in Lecture 14. You can choose $\\epsilon$ be $10^{-5}$, and $\\texttt{tol}$ be $10^{-2}$. If the gradient checking is not passed in an iteration of the gradient descent, your function should print a warning message (you do not have to `break`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
