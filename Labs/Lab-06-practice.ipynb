{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Practice Week 6\n",
    "\n",
    "This notebook does not need to be submitted. This is only for you to gain experience and get some practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent of Beale function\n",
    "Reference: Lecture 14 notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beale function is one of the [benchmark for testing your optimization algorithm](https://en.wikipedia.org/wiki/Test_functions_for_optimization):\n",
    "$$\\displaystyle f(x,y)=\\left(1.5-x+xy\\right)^{2}+\\left(2.25-x+xy^{2}\\right)^{2} \n",
    "+\\left(2.625-x+xy^{3}\\right)^{2}$$\n",
    "We know that this function has the global minimum is achieved at $(3,0.5)$. But it has lots of traps (saddle points and local minima, very flat gradient near the global minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm # display colormap in log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beale function\n",
    "f  = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical differentiation\n",
    "def numpartialx(f):\n",
    "    h = 1e-6\n",
    "    return (lambda x, y: (f(x+h, y)-f(x, y))/h)\n",
    "\n",
    "def numpartialy(f):\n",
    "    h = 1e-6\n",
    "    return (lambda x, y: (f(x, y+h)-f(x, y))/h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent in Lecture 14's notebook\n",
    "def grad_descent(f, x0 = (0,0), eta=0.01, num_steps=200):\n",
    "    # starting at some point\n",
    "    x, y = x0[0], x0[1]\n",
    "\n",
    "    x_vals = np.zeros(num_steps)\n",
    "    y_vals = np.zeros(num_steps)\n",
    "    f_vals = np.zeros(num_steps)\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        # update x and y\n",
    "        dx, dy = numpartialx(f)(x,y), numpartialy(f)(x,y)\n",
    "        x = x - eta*dx\n",
    "        y = y - eta*dy\n",
    "\n",
    "        # let's store the x, y and f(x,y) values for later use\n",
    "        x_vals[i] = x\n",
    "        y_vals[i] = y\n",
    "        f_vals[i] = f(x, y)\n",
    "    \n",
    "    return x_vals, y_vals, f_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example test\n",
    "Initial point $(x_0,y_0) = (-2,-2)$, step size $\\eta = 10^{-4}$, $2000$ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test\n",
    "num_steps = 2000\n",
    "x_vals, y_vals, f_vals = grad_descent(f, x0 = (-2,-2), eta = 1e-4, num_steps=num_steps)\n",
    "print(\"The value of f(x,y): \", f(x_vals[-1],y_vals[-1]), \"after\", \n",
    "      num_steps, \"iterations at point\", (x_vals[-1],y_vals[-1]))\n",
    "# let's see what the f(x,y) values were    \n",
    "plt.title(\"f(x,y) over gradient descent steps\")\n",
    "plt.plot(range(num_steps), f_vals)\n",
    "plt.yscale('log') # we should get a straight line under log scale\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# contour plot\n",
    "X = np.linspace(-4.5,4.5,300)\n",
    "Y = np.linspace(-4.5,4.5,300)\n",
    "X, Y = np.meshgrid(X,Y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# these 4 lines of code is plotting the contour\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "CS = ax.contour(X, Y, Z, levels=np.logspace(0, 5, 35), cmap='jet', norm = LogNorm())\n",
    "plt.axis('tight')\n",
    "ax.clabel(CS, inline=True, fontsize=8)\n",
    "\n",
    "# let's plot the arrow every few times to avoid congestion of arrows in the picture\n",
    "delta_n = 100\n",
    "for i in range(0,num_steps-delta_n,delta_n):\n",
    "    # plt.scatter(x_vals[i], y_vals[i])\n",
    "    plt.arrow(x_vals[i], y_vals[i], (x_vals[i+delta_n] - x_vals[i]), \n",
    "              (y_vals[i+delta_n] - y_vals[i]), \n",
    "              head_width=0.3, head_length=0.2, linewidth = 1.5, color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "After 2000 steps, the gradient descent still does not quite converge to the minimum at $(3,0.5)$. Try these few parameter combinations see what happened (you can copy the contour plot routine to plot the converging process)\n",
    "* $(x_0,y_0) = (4,4)$, step size $\\eta = 10^{-4}$, $2000$ steps.\n",
    "* $(x_0,y_0) = (-3,2)$, step size $\\eta = 10^{-4}$, $2000$ steps.\n",
    "* $(x_0,y_0) = (1,2)$, step size $\\eta = 10^{-3}$, $2000$ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your tests here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Try varying the step size $\\eta$, we found that when $\\eta = 10^{-4}$ the gradient descent does not converge for $(x_0,y_0) = (4,4)$. Try implement a variable step size for this initial guess so that the gradient descent will converge.\n",
    "\n",
    "HINT: decrease the step size bit by bit, making it closer and closer to zero, as number of iterations increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your tests here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 (optional)\n",
    "* Read the [`scipy.optimize.minimize` manual](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) in SciPy, and write a short script using this function to minimize Beale function with initial guess $(4,4)$ and `method` being Newton-CG.\n",
    "\n",
    "* Read the [Newton method for optimizing a function](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) which uses the information of the second derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your tests here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
