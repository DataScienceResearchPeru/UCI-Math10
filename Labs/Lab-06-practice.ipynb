{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Practice Week 6\n",
    "\n",
    "This notebook does not need to be submitted. This is only for you to gain experience and get some practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Multi-dimensional data fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem uses an artificial dataset generated by the [`make_regression` function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) in the `scikit-learn`'s `dataset` submodule. This time we have two features in each training sample. Recall in Week 6's lectures, we have learned the regressions of the citric acid (feature) vs fixed acidity (target) and volatile acidity (feature) vs fixed acidity (target). In both examples, the key problem is essentially a 1-dimensional problem since we using a line (1-d object) to \"fit\" the general trend. \n",
    "\n",
    "If we have two features, say a data point is $\\mathbf{x} = (x_1, x_2)$, and the label is $y$, then the linear regression is to find a linear function \n",
    "$$ h(\\mathbf{x}; \\mathbf{w}) = w_0 + w_1 x_1 + w_2 x_2 = \\mathbf{w}^{\\top} [1, \\mathbf{x}]$$\n",
    "\n",
    "where $\\mathbf{w}$ is a vector including the bias ($w_0$) and weights ($w_1$ and $w_2$), and we hope $h(\\mathbf{x}) \\approx y$. Suppose we totally have $N$ training samples, our loss function can be written as: denote the features for $i$-th training sample as $\\mathbf{x}^{(i)}$ and the target value for $i$-th training sample as $y^{(i)}$\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}; X,\\mathbf{y}) = \n",
    "\\frac{1}{N}\\sum_{i=1}^N  \\left( [1, \\;\\mathbf{x}^{(i)}]^{\\top} \\mathbf{w} - y^{(i)} \\right)^2\n",
    "= \\frac{1}{N}\\sum_{i=1}^N  \n",
    "\\left( w_0 x_0^{(i)} + w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right)^2,\n",
    "$$\n",
    "where $x_0^{(i)} = 1$ is articially added to each training samples as the 0-th feature.\n",
    "\n",
    "Taking gradient for $w_0$, $w_1$, and $w_2$: for $k=0,1,2$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{2}{N}\\sum_{i=1}^N x^{(i)}_k \\left(h(\\mathbf{x}^{(i)}) - y^{(i)}\\right)\n",
    "= \\frac{2}{N}\\sum_{i=1}^N x^{(i)}_k \\left(w_0 x_0^{(i)} + w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right),\n",
    "$$\n",
    "which is the sum of the product of $k$-th feature and the residual $h(\\mathbf{x}^{(i)}) - y^{(i)}$. An easier expression for vectorization is:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} \n",
    "= \\frac{2}{N}\\sum_{i=1}^N \\mathbf{x}^{(i)} \\left(w_0 x_0^{(i)} + w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero bias dataset\n",
    "\n",
    "If we already know that our dataset has zero bias, i.e., the best weight possible $w_0 = 0$, then our model simplifies to:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N  \\left(w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right)^2,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{2}{N}\\sum_{i=1}^N\\mathbf{x}^{(i)} \n",
    "\\left(w_1 x_1^{(i)} + w_2 x_2^{(i)}  - y^{(i)} \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# generate a zero bias dataset\n",
    "X, y = make_regression(n_samples=5000, n_features= 2, bias = 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape # training sample's shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape # testing target values/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn for better visualization than pyplot\n",
    "# visualization of all data\n",
    "sns.set(style='white')\n",
    "_, ax = plt.subplots(figsize=[12,8])\n",
    "sns.scatterplot(ax = ax, x = X[:,0], y = X[:,1],  s = y, \n",
    "                # s is the size of the dot, we use our target value y as the size of the dot\n",
    "                alpha=0.4, edgecolors='w')\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Training\n",
    "\n",
    "Adapt the gradient descent method from Lecture 14 to this case using the exact gradient. Implement the exact gradient (not numerical gradient) of the loss function in the cell below. When you have done training the model, compute the predicted target value using your model, then visualize the trained model's prediction using the cell which follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def h(w, X): \n",
    "    return np.matmul(X,w)\n",
    "\n",
    "# loss\n",
    "def loss(w, X, y):\n",
    "    residual = h(w, X) - y\n",
    "    return np.mean(residual**2)\n",
    "\n",
    "# gradient of the loss\n",
    "def gradient_loss(w, X, y):\n",
    "    # implemente the gradient here to replace pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your implementation of gradient descent here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the y_pred using your model\n",
    "y_pred = h(w, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(16, 8))\n",
    "# the size of the dots represents the target values\n",
    "sns.scatterplot(ax = axes[0], x = X_test[:,0], y = X_test[:,1],  s = y_pred, \n",
    "                alpha=0.4, color='b').set_title(\"Predicted value\", fontsize = 20)\n",
    "sns.scatterplot(ax = axes[1], x = X_test[:,0], y = X_test[:,1],  s = y_test, \n",
    "                alpha=0.4, color='r').set_title(\"Actual value\", fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 (optional): Gradient checking\n",
    "\n",
    "So far we have worked with relatively straightforward loss functions, and their gradients can be derived with pen-and-paper, and then be implemented directly. \n",
    "\n",
    "We also have used a forward/central difference to approximate gradient function.\n",
    "\n",
    "For more complex models that we will see later, for example the backpropagation for neural networks involving multiple nonlinear composite functions' gradient. The gradient computation can be notoriously difficult to debug and get right. Sometimes a subtle buggy implementation will manage to learn something that can look surprisingly reasonable (while performing much worse than a correct implementation). Thus, even with a buggy implementation, it may not at all be apparent that anything is amiss. \n",
    "\n",
    "In this problem, we simply check if the gradient we implemented agrees with the numerical gradient (usually the central difference) up to certain tolerance we set. This is called *gradient checking*. Carrying out the derivative checking procedure described here will significantly increase your confidence in the correctness of your code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a function $g_k(\\mathbf{w})$ that purportedly computes $\\partial L/\\partial w_k$; we would like to check if $g_k(\\mathbf{w})$ is outputting correct derivative values. Let \n",
    "\n",
    "$$\\mathbf{e}_k = (0,\\dots, 1, \\dots, 0)^{\\top},$$\n",
    "\n",
    "be the unit vector in the $k$-th component. Then the gradient checking reads:\n",
    "\n",
    ">  $\\mathbf{w}$: the weights, $\\epsilon$: certain close to 0 values, $\\texttt{Tol}$: tolerance we set<br>\n",
    "> If $\\displaystyle \\left| g_k(\\mathbf{w}) - \\frac{L(\\mathbf{w} + \\epsilon\\mathbf{e}_k) - L(\\mathbf{w} - \\epsilon\\mathbf{e}_k)}{2\\epsilon}\\right| < \\texttt{Tol}$ for every $k$ <br><br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; Proceed to do gradient decent<br><br>\n",
    "> Else<br><br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; Print a warning message to the user and do gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "Add the gradient checking algorithm to your gradient descent code, you can copy the numerical gradient implementation in Lecture 12-13. You can choose $\\epsilon$ be $10^{-5}$, and $\\texttt{tol}$ be $10^{-2}$. If the gradient checking is not passed in an iteration of the gradient descent, your function should print a warning message (you do not have to `break`).\n",
    "\n",
    "Tip: you can purposely make the gradient function wrong (for example, dividing the correct by a big constant while keeping the step size and number of iterations unchanged) to test if the gradient checking is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
